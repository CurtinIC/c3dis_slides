{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">Much of the material in this notebook has been drawn from the latest PyTables [official tutorials](http://www.pytables.org/usersguide/tutorials.html). This use is allowed under the terms of PyTables' [BSD 3-clause license](https://opensource.org/licenses/BSD-3-Clause). The tutorial material has been reformatted into Notebook format and changes have been made to the text in some places. So this notebook could be accurately called a fork of the PyTables tutorials.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [Background](#Background)\n",
    "- [A PyTables Glossary](#A-PyTables-Glossary)\n",
    "- [Our First HDF5 File](#Our-First-HDF5-File)\n",
    "- [Browsing the Data Tree](#Browsing-the-Data-Tree)\n",
    "- [Multidimensional Data](#Multidimensional-Data)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "[The Hierachical Data Format (HDF)](https://www.hdfgroup.org/) is designed to store and organise large amounts of data in a hierarchy of groups and datasets, along with descriptive metadata. HDF is self-describing. Metadata in the file allows applications to interpret the structure and contents of a file without reference to outside information.\n",
    "\n",
    "## PyTables\n",
    "It is hard to do better than the [official documentation](http://www.pytables.org/index.html) for a description:\n",
    "\n",
    "> PyTables is a package for managing hierarchical datasets and designed to efficiently and easily cope with extremely large amounts of data.\n",
    "\n",
    ">PyTables is built on top of the HDF5 library, using the Python language and the NumPy package. It features an object-oriented interface that, combined with C extensions for the performance-critical parts of the code (generated using Cython), makes it a fast, yet extremely easy to use tool for interactively browse, process and search very large amounts of data. \n",
    "\n",
    "## PyTables vs h5py\n",
    "At present, there are two main Python libraries for working with HDF5 data: [PyTables](http://www.pytables.org/) and [h5py](http://www.h5py.org/). h5py follows the underlying HDF5 API closely, mapping the HDF5 data to numpy data structures. PyTables provides a higher-level, database-like approach to data storage, with features such as advanced indexing, fast queries, undo/redo, and enriched (compared to NumPy/h5py) data types.\n",
    "\n",
    "For more information on PyTables vs h5py, you can read both the [PyTables](http://www.pytables.org/FAQ.html#how-does-pytables-compare-with-the-h5py-project) and [h5py](http://docs.h5py.org/en/latest/faq.html#what-s-the-difference-between-h5py-and-pytables) sides of the story. Work is in progress to unify the efforts of h5py and PyTables, although this work has not yet reached the main release. This effort will see PyTables being built on top of h5py rather than independently producing bindings directly to the HDF5 API. See:\n",
    "- [PyTables: A New Backend Interface](https://github.com/PyTables/PyTables/blob/pt4/doc/New-Backend-Interface.rst)\n",
    "- [Python and HDF5 - A Vision](https://www.hdfgroup.org/2015/09/python-hdf5-a-vision/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A PyTables Glossary\n",
    "HDF5 files are organised into a hierarchical tree-like structure. Starting from the file, nodes are used to represent each item in the tree. PyTables contains a number of different node types including groups, tables, and arrays. \n",
    "\n",
    "**File**: The file is the basic unit of storage for HDF5 and PyTables. The entire data hierarchy is stored in a single, often large, file on disk (note I am glossing over the external link functionality which lets you create a link from a node to an external file).\n",
    "\n",
    "The PyTables interface to the HDF5 file is provided by the [`File`](http://www.pytables.org/usersguide/libref/file_class.html) class.\n",
    "\n",
    "**Node**: Each element in the hierarchy is represented by a node. Three features of interest are:\n",
    "- root node: This sits at the top of the hierarchy. This node is always present, even in an empty file. It can be accessed by the attribute `File.root`.\n",
    "- Paths: Every node in the hierarchy has a path. Similar to files in a file system, the path is formed by concatenating the names of parent nodes, separated by a `/`. Paths start with `/` to represent the root node.\n",
    "- There are two main types of node: groups and leaves.\n",
    "- Nodes can contain metadata.\n",
    "\n",
    "**Groups**: PyTables uses the [`Group`](http://www.pytables.org/usersguide/libref/hierarchy_classes.html#the-group-class) class to organise the data. Instances of this class are grouping structures containing child instances of zero or more groups or leaves, together with supporting metadata. Each group has exactly one parent group.\n",
    "\n",
    "**Leaves**: Leaf nodes sit inside a group node, but unlike a group they cannot have any further children. This is similar to files in a file system.\n",
    "\n",
    "**Tables**: The [`Table`](http://www.pytables.org/usersguide/libref/structured_storage.html#the-table-class) class allows storage of heterogeneous tabular data in a HDF5 file. Table data consists of a unidimensional sequence of rows, where each row contains one or more fields. Fields have an associated unique name and position, with the first field having position 0. All rows have the same fields, which are arranged in columns. Tables are leaf nodes.\n",
    "\n",
    "**Arrays**: Arrays allow the storage of multidimensional homogenous data in a HDF5 file. The main PyTables class is the [`Array`](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-array-class), although other classes are available for enabling [data compression](http://www.pytables.org/usersguide/libref/homogenous_storage.html#carrayclassdescr), [resizable arrays](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-earray-class), and [ragged arrays](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-vlarray-class). If you have previously used HDF5 or NetCDF, then PyTables arrays will be the most familiar mechanism for storing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our First HDF5 File\n",
    "In this section, we will see how to define our own records in Python and save collections of them (i.e. a table) into a file. Then we will select some of the data in the table using Python cuts and create NumPy arrays to store this selection as separate objects in a tree.\n",
    "\n",
    "First, import PyTables and Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tables as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Data\n",
    "Now, imagine that we have a particle detector and we want to create a table object in order to save data retrieved from it. You need first to define the table, the number of columns it has, what kind of object is contained in each column, and so on.\n",
    "\n",
    "Our particle detector has a TDC (Time to Digital Converter) counter with a dynamic range of 8 bits and an ADC (Analogical to Digital Converter) with a range of 16 bits. For these values, we will define 2 fields in our record object called `TDCcount` and `ADCcount`. We also want to save the grid position in which the particle has been detected, so we will add two new fields called `grid_i` and `grid_j`. Our instrumentation also can obtain the pressure and energy of the particle. The resolution of the pressure-gauge allows us to use a single-precision float to store pressure readings, while the energy value will need a double-precision float. Finally, to track the particle we want to assign it a name to identify the kind of the particle it is and a unique numeric identifier. So we will add two more fields: name will be a string of up to 16 characters, and idnumber will be an integer of 64 bits (to allow us to store records for extremely large numbers of particles).\n",
    "\n",
    "Having determined our columns and their types, we can now declare a new Particle class that will contain all this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Particle(pt.IsDescription):\n",
    "    name      = pt.StringCol(16)   # 16-character String\n",
    "    idnumber  = pt.Int64Col()      # Signed 64-bit integer\n",
    "    ADCcount  = pt.UInt16Col()     # Unsigned short integer\n",
    "    TDCcount  = pt.UInt8Col()      # unsigned byte\n",
    "    grid_i    = pt.Int32Col()      # 32-bit integer\n",
    "    grid_j    = pt.Int32Col()      # 32-bit integer\n",
    "    pressure  = pt.Float32Col()    # float  (single-precision)\n",
    "    energy    = pt.Float64Col()    # double (double-precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare a class variable for each field, assigning an instance of the appropriate Col subclass, according to the required column attributes (the data type, the length, the shape, etc). See the [The Col class and its descendants](http://www.pytables.org/usersguide/libref/declarative_classes.html#colclassdescr) for a complete description of these subclasses. See also [Supported data types in PyTables](http://www.pytables.org/usersguide/datatypes.html#datatypes) for a list of data types supported by the Col constructor.\n",
    "\n",
    "From now on, we can use Particle instances as a descriptor for our detector data table. We will see later on how to pass this object to construct the table. But first, we must create a file where all the actual data pushed into our table will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a PyTables File\n",
    "Use the top-level `open_file()` function to create a PyTables file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?pt.open_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file = pt.open_file(\n",
    "    filename='tutorial1.h5',  # File name (will be created)\n",
    "    mode='w',  # Create in write mode\n",
    "    title='Test file')  # Our first metadata - a descriptive title for the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5file.root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function attempts to open the file, and if successful, returns the `File` (see [The File Class](http://www.pytables.org/usersguide/libref/file_class.html#fileclassdescr)) object instance `h5file`. The root of the object tree is specified in the instance's root attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new group\n",
    "Now, to better organize our data, we will create a group called detector that branches from the root node. We will save our particle data table in this group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?pt.File.create_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group = h5file.create_group(\n",
    "    where='/',\n",
    "    name='detector',\n",
    "    title='Detector information')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have taken the `File` instance h5file and invoked its `File.create_group()` method to create a new group called `detector` branching from \"/\" (another way to refer to the h5file.root object we mentioned above). This will create a new `Group` (see [The Group class](http://www.pytables.org/usersguide/libref/hierarchy_classes.html#groupclassdescr)) object instance that will be assigned to the variable group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a New Table\n",
    "Let’s now create a `Table` (see [The Table class](http://www.pytables.org/usersguide/libref/structured_storage.html#tableclassdescr)) object as a branch off the newly-created group. We do that by calling the `File.create_table()` method of the h5file object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?pt.File.create_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = h5file.create_table(where=group, name='readout', description=Particle, title='Readout example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so now we have created a table based on the `Particle` class, under the 'detector' group. We can examine the file structure by printing the `File` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(h5file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information, including the column datatypes for each table can also be displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(h5file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print just the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think that `Table(0,)` means? \n",
    "\n",
    "**Hint:** try printing the table again after adding some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to the Table\n",
    "We can now start adding data to the readout table. First we get a reference to the [`Row`](http://www.pytables.org/usersguide/libref/structured_storage.html#rowclassdescr) handle for the table. Data for each column and row can then be written to the `Row` object as though it was a dictionary, with keys corresponding to the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "particle = table.row\n",
    "particle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `Row` instance keeps track of the current row. Calling `Row.append()` saves the current data and moves the internal reference to a new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    particle['name']  = 'Particle: %6d' % (i)\n",
    "    particle['TDCcount'] = i % 256\n",
    "    particle['ADCcount'] = (i * 256) % (1 << 16)\n",
    "    particle['grid_i'] = i\n",
    "    particle['grid_j'] = 10 - i\n",
    "    particle['pressure'] = float(i*i)\n",
    "    particle['energy'] = float(particle['pressure'] ** 4)\n",
    "    particle['idnumber'] = i * (2 ** 34)\n",
    "    # Insert a new particle record\n",
    "    particle.append()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have processed all our data, we should flush the table’s I/O buffer if we want to write all this data to disk. We achieve that by calling the `table.flush()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.flush()\n",
    "table.close()\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, flushing a table is a very important step as it will not only help to maintain the integrity of your file, but also will free valuable memory resources (i.e. internal buffers) that your program may need for other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Selecting Data in a Table\n",
    "Let's have a look at our current working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tutorial1.h5` contains the table of particle detector readouts. So let's open it in `r+`-mode (read-write mode, but the file must already exist) and read some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5file = pt.open_file(\n",
    "    filename='tutorial1.h5',\n",
    "    mode='r+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to check the file structure, just print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(h5file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the readout table containing the particle detector readings that we need. Let's read it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = h5file.root.detector.readout  # create an alias to the readout table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Table` class supports iteration by rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in table:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not really what we expected. Recall that the `Row` class is a PyTables class, not the `Particle` class we defined earlier. To access the row data, you can call the `Row.fetch_all_fields()` method, which returns a tuple of all field data as Numpy scalar types. You can also use slice syntax, but in this case the fields are returned as native Python types. See the [Row documentation](http://www.pytables.org/usersguide/libref/structured_storage.html#rowclassdescr) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in table:\n",
    "    print(row.fetch_all_fields())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields can also be retrieved by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in table:\n",
    "    print(row['TDCcount'], ':', row['pressure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to check the column names, you can either refer to the column definition class (`Particle`), or query the table `colnames` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often though, you don't want to retrieve an entire dataset. PyTables provides efficient ways to query and filter the data. Let's select all pressure values for observations where $TDCcount>3$ and $20 <= pressure < 50$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pressure = [x['pressure'] for x in table if x['TDCcount'] > 3 and 20 <= x['pressure'] < 50]\n",
    "pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions work well for small data sets, but PyTables provides additional search functionality that are more appropriate for large tables or where query speed is critical. They are called *in-kernel* and *indexed* queries, and you can use them through `Table.where()` and other related methods.\n",
    "\n",
    "Let’s repeat the pressure query with an in-kernel method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pressure = [x['pressure'] for x in table.where('(TDCcount > 3) & (pressure >= 20) & (pressure < 50)')]\n",
    "pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this functionality is built on top of the NumExpr library. If you have used this library before (directly or perhaps through `pandas.eval`), you will recall that the syntax is not exactly the same as pure Python. Notice the use of `&` instead of `and`, as well as the parentheses around each term. \n",
    "\n",
    " See [Condition Syntax](http://www.pytables.org/usersguide/condition_syntax.html#condition-syntax) and [Accelerating your searches](http://www.pytables.org/usersguide/optimization.html#searchoptim) for more information on in-kernel and indexed selections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings Require Special Care\n",
    "<div class=\"alert alert-danger\">\n",
    "Recall that in the `Particle` definition, we defined `name` as a 16 character string (`StringCol(16)`)? PyTables stores this as a byte-array and not a string. Among other things, this means that queries on string columns need special handling.\n",
    "</div>\n",
    "\n",
    "First, let's check the column types through the table object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.coltypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says that `name` is a `string`. Let's retrieve the first item from the name column and examine the actual type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(table.cols.name[0])\n",
    "display(type(table.cols.name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this affects a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in table.where('(name == \"Particle:      5\") | (name == \"Particle:      7\")'):\n",
    "    print(row['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python 2 the previous query will work, but in Python 3 it fails since the a unicode literal cannot be compared to a numpy byte array. To build a string query that works on all Python versions, you need to specify byte array literals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in table.where('(name == b\"Particle:      5\") | (name == b\"Particle:      7\")'):\n",
    "    print(row['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Data to a Table\n",
    "In order to separate the selected data from the mass of detector data, we will create a new group columns branching off the root group. Afterwards, under this group, we will create two arrays that will contain the selected pressure and name data.\n",
    "\n",
    "Note that the new arrays are not a dynamic query. If the main particle data changes later, the new data will not automatically reflect the changes.\n",
    "\n",
    "First, we create the group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gcolumns = h5file.create_group(\n",
    "    where=h5file.root,  # Note that we give a reference to the parent group, instead of the string path \"/\"\n",
    "    name=\"columns\",\n",
    "    title=\"Pressure and Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the pressure array using the `File.create_array()` method, converting the list of pressure results to a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5file.create_array(\n",
    "    where=gcolumns,\n",
    "    name='pressure',\n",
    "    obj=np.array(pressure),  # The data to be saved into the array, converted from list to numpy array\n",
    "    title=\"Pressure column selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the array for names. In this case we store the Python list as-is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [x['name'] for x in table.where('(TDCcount > 3) & (pressure >= 20) & (pressure < 50)')]\n",
    "h5file.create_array(\n",
    "    where=gcolumns,\n",
    "    name='name',\n",
    "    obj=names,\n",
    "    title=\"Name column selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `File.create_array()` accepts names (which is a regular Python list) as the `obj` parameter. Actually, it accepts a variety of different regular objects as parameters. The flavor attribute (see the output above) saves the original object type so that PyTables will be able to retrieve exactly the same object from disk later on.\n",
    "\n",
    "Now lets examine the current file structure to confirm that our new arrays are there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(h5file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the File and Examining the Contents\n",
    "First, let's close the file (closing also flushes the file to disk first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now created your first PyTables file with a table and two arrays. You can examine it with any generic HDF5 tool, such as [h5dump](https://support.hdfgroup.org/HDF5/Tutor/cmdtoolview.html#dh5dump) or [h5ls](https://support.hdfgroup.org/HDF5/Tutor/cmdtoolview.html#h5ls). Here is what `tutorial1.h5` looks like when read with the h5ls program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!h5ls -r tutorial1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the PyTables command-line utility `ptdump`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ptdump tutorial1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ptdump` understands the PyTables metadata in the file, and gives more information about how the data will appear to PyTables compared to generic utilities.\n",
    "\n",
    "Note that h5ls described both '/columns/name' and '/detector/readout' as datasets, while ptdump understands that one is a PyTables array and the other is a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Browsing the Data Tree\n",
    "In this section we will learn how to browse the HDF5 data tree, as well as reading and writing data and metadata.\n",
    "## Traversing the Tree\n",
    "Let's start by opening the file from the last section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5file = pt.open_file('tutorial1.h5', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a preliminary overview of the object tree by simply printing the existing `File` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(h5file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s make use of the `File` iterator to see how to list all the nodes in the object tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for node in h5file:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shows all nodes (`RootGroup`, `Group`, `Array`, and `Table` in this case), and is equivalent to calling `File.walk_nodes()`.\n",
    "\n",
    "There are two basic methods for examining the tree structure. `File.walk_nodes()` performs an in-order recursive traversal of the tree. `File.iter_nodes()` performs a non-recursive traversal of a single node. Both methods accept an optional argument `where` indicating the starting node, and an optional argument `classname` indicating the specific node types to return.\n",
    "\n",
    "Let's look at a few uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a in h5file.walk_nodes(classname='Array'):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in h5file.walk_nodes(classname='Table'):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just the children of the \"Columns\" group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in h5file.iter_nodes(where='/columns'):\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, all the [leaf](http://www.pytables.org/usersguide/libref/hierarchy_classes.html#leafclassdescr) nodes in the detector group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in h5file.walk_nodes(h5file.root.detector, 'Leaf'):\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are wondering, a leaf node is simply any node that does not (and often cannot) have children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Metadata\n",
    "PyTables provides an easy and concise way to complement the meaning of your node objects on the tree by using the `AttributeSet` class (see [The AttributeSet class](http://www.pytables.org/usersguide/libref/declarative_classes.html#attributesetclassdescr)). You can access this object through the standard attribute `attrs` in `Leaf` nodes and `_v_attrs` in `Group` nodes.\n",
    "\n",
    "For example, let’s imagine that we want to save the date indicating when the data in the */detector/readout* table has been acquired, as well as the temperature during the gathering process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = h5file.root.detector.readout\n",
    "table.attrs.gath_date = \"Wed, 06/12/2003 18:33\"\n",
    "table.attrs.temperature = 18.4\n",
    "table.attrs.temp_scale = \"Celsius\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving specific attribute values is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.attrs.temp_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting attributes is also simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del table.attrs.gath_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the current set of attributes through `Table.attrs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(table.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, that displays all attributes including the PyTables system attributes. You get more fine-grained control with the `AttributeSet._f_list()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.attrs._f_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.attrs._f_list('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.attrs._f_list('user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.attrs._f_list('sys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating attributes while retrieving their name and value is slightly less elegant, but possible. You iterate by name as shown above, and then lookup the value by name on the attribute set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in table.attrs._f_list():\n",
    "    print(\"{0}: {1}\".format(name, table.attrs[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also rename attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.attrs._f_rename('temp_scale', 'tempScale')\n",
    "table.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we flush the file now, we can check with an external tool to see that the new metadata is stored in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5file.flush()\n",
    "!h5ls -v tutorial1.h5/detector/readout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting `object` Metadata\n",
    "Each object in PyTables has metadata information about the data in the file. Normally this is accessible through the node instance variables. Let's take a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Object:\", table)\n",
    "print(\"Table name:\", table.name)\n",
    "print(\"Table title:\", table.title)\n",
    "print(\"Number of rows in table:\", table.nrows)\n",
    "print(\"Table variable names with their type and shape:\")\n",
    "for name in table.colnames:\n",
    "    print(name, ':= %s, %s' % (table.coldtypes[name], table.coldtypes[name].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `table.coldtypes` is a dictionary mapping the name of each column to the corresponding Numpy `dtype`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's retrieve the /columns/pressure Array object and look at the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pressureObject = h5file.get_node(\"/columns/pressure\")\n",
    "pressureObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"shape: ==>\", pressureObject.shape)\n",
    "print(\"title: ==>\", pressureObject.title)\n",
    "print(\"atom:  ==>\", pressureObject.atom)\n",
    "print(\"dtype:  ==>\", pressureObject.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from an Array\n",
    "You can use the `Array.read()` method to retrieve the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pressureArray = pressureObject.read()\n",
    "display(pressureArray)\n",
    "type(pressureArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `read()` returned a numpy array. Recall that PyTables stores type information for each node in the system attribute `FLAVOR`. It uses this metadata to automatically return the same data type that was stored. For example, recall that we stored */columns/name* as a Python `list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(h5file.get_node(\"/columns/name\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending Data to an Existing Table\n",
    "Adding new rows to a table is done in the same way as the initial table creation. First find the table node, then get the row iterator, append the data and finally flush the table.\n",
    "\n",
    "Now let's append some new rows to the readout table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = h5file.root.detector.readout\n",
    "particle = table.row\n",
    "for i in range(10, 15):\n",
    "    particle['name']  = 'Particle: %6d' % (i)\n",
    "    particle['TDCcount'] = i % 256\n",
    "    particle['ADCcount'] = (i * 256) % (1 << 16)\n",
    "    particle['grid_i'] = i\n",
    "    particle['grid_j'] = 10 - i\n",
    "    particle['pressure'] = float(i*i)\n",
    "    particle['energy'] = float(particle['pressure'] ** 4)\n",
    "    particle['idnumber'] = i * (2 ** 34)\n",
    "    particle.append()\n",
    "table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work, the file must have been opened in one of the append modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in table:\n",
    "    print(row.fetch_all_fields())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Existing Table Data\n",
    "We will start modifying single cells in the first row of the Particle table by indexing into the corresponding columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Before modif-->\", table[0])\n",
    "table.cols.TDCcount[0] = 1\n",
    "print(\"After modifying first row of ADCcount-->\", table[0])\n",
    "table.cols.energy[0] = 2\n",
    "print(\"After modifying first row of energy-->\", table[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify complete ranges of columns as well. Note that PyTables slicing notation generally follows the numpy convention of `object[start:stop:step]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.cols.TDCcount[2:5] = [2,3,4]\n",
    "print(\"After modifying slice [2:5] of TDCcount-->\")\n",
    "print(table[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.cols.energy[1:9:3] = [2,3,4]\n",
    "print(\"After modifying slice [1:9:3] of energy-->\")\n",
    "print(table[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is a way to modify table data using the `Row` accessor that we have used for appending rows. This can be combined with table queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(table.cols.energy[0:4])\n",
    "\n",
    "for row in table.where('TDCcount <= 2'):\n",
    "    row['energy'] = row['TDCcount'] * 2\n",
    "    row.update()\n",
    "    \n",
    "print(table.cols.energy[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Table Rows\n",
    "Use the `Table.remove_rows()` method. It deletes rows in the semi-closed range [start, stop) (start index is included, stop index is not). For example, delete rows 5 to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.remove_rows(5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`remove_rows()` returns the number of removed rows.\n",
    "\n",
    "Single rows can also be removed with `Table.remove_row()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table.remove_row(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Existing Array Data\n",
    "Let’s see at how modify data on the pressureObject array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pressureObject = h5file.root.columns.pressure\n",
    "print(\"Before modif-->\", pressureObject[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pressureObject[0] = 2\n",
    "print(\"First modif-->\", pressureObject[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pressureObject[1:3] = [2.1, 3.5]\n",
    "print(\"Second modif-->\", pressureObject[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pressureObject[::2] = [1,2]\n",
    "print(\"Third modif-->\", pressureObject[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you can use any combination of (multidimensional) extended slicing.\n",
    "\n",
    "With the sole exception that you cannot use negative values for step to refer to indexes that you want to modify. See [`Array.__getitem__()`](http://www.pytables.org/usersguide/libref/homogenous_storage.html#tables.Array.__getitem__) for more examples on how to use extended slicing in PyTables objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is now complete, so close the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Data\n",
    "Now it’s time for a more real-life example (i.e. with errors in the code). We will create two groups that branch directly from the root node, Particles and Events. Then, we will put three tables in each group. In Particles we will put tables based on the `Particle` descriptor and in Events, the tables based on the `Event` descriptor.\n",
    "\n",
    "Afterwards, we will provision the tables with a number of records. Finally, we will read the newly-created table /Events/TEvent3 and select some values from it, using a list comprehension.\n",
    "\n",
    "We also introduce a new manner to describe a Table as a structured NumPy dtype (or even as a dictionary), as you can see in the Event description. See [`File.create_table()`](http://www.pytables.org/usersguide/libref/file_class.html#tables.File.create_table) about the different kinds of descriptor objects that can be passed to this method.\n",
    "\n",
    "This section uses a different `Particle` definition to the earlier sections, so let's define it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Particle(pt.IsDescription):\n",
    "    name        = pt.StringCol(itemsize=16)  # 16-character string\n",
    "    lati        = pt.Int32Col()              # integer\n",
    "    longi       = pt.Int32Col()              # integer\n",
    "    pressure    = pt.Float32Col(shape=(2,3)) # array of floats (single-precision)\n",
    "    temperature = pt.Float64Col(shape=(2,3)) # array of doubles (double-precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the `Event` table. We could do this in the same manner as `Particle`, but here we demonstrate the use of a Numpy `dtype` structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Event = np.dtype([\n",
    "    (\"name\"     , \"S16\"),\n",
    "    (\"TDCcount\" , np.uint8),\n",
    "    (\"ADCcount\" , np.uint16),\n",
    "    (\"xcoord\"   , np.float32),\n",
    "    (\"ycoord\"   , np.float32)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new file in \"w\"rite mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileh = pt.open_file(\"tutorial2.h5\", mode = \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the HDF5 root group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = fileh.root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for groupname in (\"Particles\", \"Events\"):\n",
    "    group = fileh.create_group(root, groupname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">**Note:** The following two code cells contain deliberate errors. Please experiment with changing the code to explore some of the sanity checking that PyTables performs.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create and fill the tables in Particles group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 3 new tables\n",
    "for tablename in (\"TParticle1\", \"TParticle2\", \"TParticle3\"):\n",
    "    # Create a table, or retrieve it if the table already exists (so this code cell can be executed multiple times)\n",
    "    try:\n",
    "        table = fileh.create_table(\"/Particles\", tablename, Particle, \"Particles: \" + tablename)\n",
    "    except pt.NodeError:\n",
    "        table = fileh.get_node(root.Particles, tablename)\n",
    "\n",
    "    # Get the record object associated with the table:\n",
    "    particle = table.row\n",
    "\n",
    "    # Fill the table with 257 particles\n",
    "    for i in range(257):\n",
    "        # First, assign the values to the Particle record\n",
    "        particle['name'] = 'Particle: %6d' % (i)\n",
    "        particle['lati'] = i\n",
    "        particle['longi'] = 10 - i\n",
    "\n",
    "        ########### Detectable errors start here. Play with them!\n",
    "        #particle['pressure'] = np.array(i*np.arange(2*3)).reshape((2,4))  # Incorrect\n",
    "        particle['pressure'] = np.array(i*np.arange(2*3)).reshape((2,3)) # Correct\n",
    "        ########### End of errors\n",
    "\n",
    "        particle['temperature'] = (i**2)     # Broadcasting\n",
    "\n",
    "        # This injects the Record values\n",
    "        particle.append()\n",
    "\n",
    "    # Flush the table buffers\n",
    "    table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the Events group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tablename in (\"TEvent1\", \"TEvent2\", \"TEvent3\"):\n",
    "    # Create or retrieve the table\n",
    "    try:\n",
    "        table = fileh.create_table(root.Events, tablename, Event, \"Events: \" + tablename)\n",
    "    except pt.NodeError:\n",
    "        table = fileh.get_node(root.Events, tablename)\n",
    "\n",
    "    # Get the record object associated with the table:\n",
    "    event = table.row\n",
    "\n",
    "    # Fill the table with 257 events\n",
    "    for i in range(257):\n",
    "        # First, assign the values to the Event record\n",
    "        event['name']  = 'Event: %6d' % (i)\n",
    "        event['TDCcount'] = i % (1<<8)   # Correct range\n",
    "\n",
    "        ########### Detectable errors start here. Play with them!\n",
    "        #event['xcoor'] = float(i**2)     # Wrong spelling\n",
    "        event['xcoord'] = float(i**2)   # Correct spelling\n",
    "        #event['ADCcount'] = \"sss\"        # Wrong type\n",
    "        event['ADCcount'] = i * 2       # Correct type\n",
    "        ########### End of errors\n",
    "\n",
    "        event['ycoord'] = float(i)**4\n",
    "\n",
    "        # This injects the Record values\n",
    "        event.append()\n",
    "\n",
    "    # Flush the buffers\n",
    "    table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape Checking\n",
    "\n",
    "One of the preceeding errors looked like this:\n",
    "\n",
    "```python\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-23-70395748746d> in <module>()\n",
    "     18 \n",
    "     19         ########### Detectable errors start here. Play with them!\n",
    "---> 20         particle['pressure'] = np.array(i*np.arange(2*3)).reshape((2,4))  # Incorrect\n",
    "     21         #particle['pressure'] = np.array(i*np.arange(2*3)).reshape((2,3)) # Correct\n",
    "     22         ########### End of errors\n",
    "\n",
    "ValueError: cannot reshape array of size 6 into shape (2,4)\n",
    "```\n",
    "\n",
    "This error indicates that you are trying to assign an array with an incompatible shape to a table cell. Looking at the code, we see that we were trying to assign an array of shape (2,4) to a pressure element, which was defined with the shape (2,3).\n",
    "\n",
    "In general, these kinds of operations are forbidden, with one valid exception: when you assign a scalar value to a multidimensional column cell, all the cell elements are populated with the value of the scalar. For example:\n",
    "\n",
    "```python\n",
    "particle['temperature'] = (i**2)    # Broadcasting\n",
    "```\n",
    "\n",
    "The value `i**2` is assigned to all the elements of the temperature table cell. This capability is provided by the NumPy package and is known as broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Name Checking\n",
    "Another error was the `KeyError`:\n",
    "```python\n",
    "KeyError: 'no such column: xcoor'\n",
    "```\n",
    "This error indicates that we are attempting to assign a value to a non-existent field in the event table object. By looking carefully at the Event class attributes, we see that we misspelled the xcoord field (we wrote xcoor instead). This is unusual behavior for Python, as normally when you assign a value to a non-existent instance variable, Python creates a new variable with that name. Such a feature can be dangerous when dealing with an object that contains a fixed list of field names. PyTables checks that the field exists and raises a `KeyError` if the check fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Checking\n",
    "Finally, the last issue was a `TypeError` exception:\n",
    "```python\n",
    "TypeError: invalid type (<class 'str'>) for column ``ADCcount``\n",
    "```\n",
    "\n",
    "This is because we defined the `ADCcount` column as type `np.uint16`, so assigning a string value is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root.Events.TEvent1.coldtypes['ADCcount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "Assuming you fixed the errors, let's close the file and examine the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ptdump -v tutorial2.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to Now?\n",
    "PyTables is a large library, with many advanced features that were not covered in this notebook. Good next steps are:\n",
    "- The [Using PyTables for Larger-Than-RAM Data Processing](https://kastnerkyle.github.io/posts/using-pytables-for-larger-than-ram-data-processing)\n",
    "blog post. I have placed a copy of the source notebook for this blog [here](/Additional%20Notebooks/Using%20PyTables%20for%20Larger-Than-RAM%20Data%20Processing/Using%20PyTables%20for%20Larger-Than-RAM%20Data%20Processing.ipynb), with some edits to adapt it to more recent PyTables versions (if you download and run the notebook from the online source, it will give a lot of deprecated function warnings). This blog gives a good introduction to working with large data sets, including the use of the chunking and compression features of HDF5.\n",
    "- The official [PyTables Tutorial](http://www.pytables.org/usersguide/tutorials.html), particularly [the second half](http://www.pytables.org/usersguide/tutorials.html#using-links-for-more-convenient-access-to-nodes) which has not been covered here.\n",
    "- [The PyTables Documentation](http://www.pytables.org/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [PyTables](http://www.pytables.org)\n",
    "- [h5py](http://www.h5py.org)\n",
    "- [HDF5](https://www.hdfgroup.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
