{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "This notebook is an edited version of the source notebook attached to the [Using PyTables for Larger-Than-RAM Data Processing](https://kastnerkyle.github.io/posts/using-pytables-for-larger-than-ram-data-processing)\n",
    "blog post. The licensing is not specified, so I am assuming that reusing this work with attribution is OK. However, please do not distribute this notebook publicly.\n",
    "\n",
    "The following types of changes have been made:\n",
    "- Updated to more recent PyTables versions to remove the deprecated function warnings.\n",
    "- Corrected a number of spelling mistakes and typos.\n",
    "- Added additional explanation, either through Markdown or code comments, where I felt things were a little unclear.\n",
    "- Some formatting changes for more consistency.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTables for Larger-Than-RAM Data Processing\n",
    "HDF5 is a direct, easy path to \"big\" (or just annoyingly larger than RAM) data in scientific python. Using the HDF5 file format and careful coding of your algorithms, it is quite possible to process \"big-ish\" data on modest hardware, and really push your resources to the limit before moving to larger, fancier distributed platforms. \n",
    "\n",
    "There are two primary options for working with HDF5 files in python: H5Py and [PyTables](http://www.pytables.org/). While many people enjoy H5Py, I am much more familiar with PyTables and prefer it for most tasks. This blog will show a variety of PyTables examples for several potential applications including standard numpy `nd-array` replacement, variable length (ragged) arrays, iterator rewriting for simpler retrieval of data, and on-the-fly compression/decompression.\n",
    "<!-- TEASER_END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our various techniques for storing data in HDF5, first we need some data to store. Here I have created a quick function which will generate random data from four clusters, as well as returning the cluster label for each datapoint. This data generator should be sufficient for testing all the PyTables techniques I will show in this post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(2014)\n",
    "\n",
    "def make_random_cluster_points(n_samples, random_state=random_state):\n",
    "    mu_options = np.array([(-1, -1), (1, 1), (1, -1), (-1, 1)])\n",
    "    sigma = 0.2\n",
    "    mu_choices = random_state.randint(0, len(mu_options), size=n_samples)\n",
    "    means = mu_options[mu_choices]\n",
    "    return means + np.random.randn(n_samples, 2) * sigma, mu_choices\n",
    "\n",
    "def plot_clusters(data, clusters, name):\n",
    "    plt.figure()\n",
    "    colors = [\"#9b59b6\", \"#3498db\", \"#e74c3c\", \"#2ecc71\"]\n",
    "    for i in np.unique(clusters):\n",
    "        plt.scatter(data[clusters==i, 0], data[clusters==i, 1], color=colors[i])\n",
    "    plt.axis('off')\n",
    "    plt.title('Plot from %s' % name)\n",
    "\n",
    "data, clusters = make_random_cluster_points(10000)\n",
    "plot_clusters(data, clusters, \"data in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a brief high level discussion of how HDF5 and PyTables work. I think the website says it best:\n",
    "\n",
    ">PyTables is a package for managing hierarchical datasets and designed to efficiently and easily cope with extremely large amounts of data. You can download PyTables and use it for free [...] One important feature of PyTables is that it optimizes memory and disk resources so that data takes much less space (specially if on-flight compression is used) than other solutions such as relational or object oriented databases.\n",
    "\n",
    "PyTables allows us to quickly and easily deal with large volumes of on-disk data, while largely keeping the complexity of the data storage invisible to the downstream processing in numpy, scipy, theano, etc. Depending on the algorithms used downstream, you can sometimes drop in hdf5 data to replace a numpy array and everything will keep working exactly like before - when this happens it is quite an amazing feeling.\n",
    "\n",
    "First we will start with the simplest use for HDF5 - data persistence. We will show this using two types of [Arrays](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-array-class) provided by PyTables - `Array` and `CArray`.\n",
    "\n",
    "We take the following steps below:\n",
    "1. Importing PyTables with `import tables`\n",
    "2. Creating some data\n",
    "3. Creating a new hdf5 file (or overwriting the old one at the same place!)\n",
    "4. Adding a PyTables `Array` to hold the data, and storing our data in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1: import PyTables\n",
    "import tables\n",
    "\n",
    "# 2: create some data\n",
    "sample_data, sample_clusters = make_random_cluster_points(10000)\n",
    "\n",
    "# 3: create a new HDF5 file\n",
    "hdf5_path = \"my_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='w') as hdf5_file:\n",
    "    # 4: create 2 PyTables arrays to store the data\n",
    "    data_storage = hdf5_file.create_array(hdf5_file.root, 'data', sample_data)\n",
    "    clusters_storage = hdf5_file.create_array(hdf5_file.root, 'clusters', sample_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can open the file, and use the data in it for the same things that we used the old arrays for. Make sure to notice that we are slicing (using [:]) the ``data`` and ``clusters`` back into memory so that we can use these variables for plotting. More on this in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='r') as read_hdf5_file:\n",
    "    # Here we slice [:] all the data back into memory, then operate on it\n",
    "    hdf5_data = read_hdf5_file.root.data[:]\n",
    "    hdf5_clusters = read_hdf5_file.root.clusters[:]\n",
    "\n",
    "plot_clusters(hdf5_data, hdf5_clusters, \"PyTables Array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why did we need to slice the data in order to use it? This comes from the way that PyTables manages on-disk versus in memory data. \n",
    "\n",
    "A PyTables array lives on disk until some data is sliced out using standard numpy notation. At that point, the data slice is read into memory from the disk, for the lifetime of that slice. Once the slice is finished being used, the python garbage collector will try to free the memory of the slice and any changes (if you made them) can be written back to the PyTables array on disk.\n",
    "\n",
    "This makes PyTables a nearly transparent drop in for numpy arrays which are larger than memory but only need to be processed a little bit at a time. For my own usage, I typically have some separate scripts to preprocess and write data to an hdf5 file, and all algorithmic processing scripts open this as read only - this is very similar to what we have seen above. \n",
    "\n",
    "# Compression using CArray\n",
    "\n",
    "Now that we have seen a basic array, we can take a look at using a compressed array, the [`CArray`](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-carray-class) class. PyTables makes it really easy to store and read compressed data from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create some test data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_data, sample_clusters = make_random_cluster_points(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a file and write the data. The main change is in how we create the arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_compressed_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='w') as hdf5_file:\n",
    "    filters = tables.Filters(complevel=5, complib='blosc')\n",
    "    data_storage = hdf5_file.create_carray(hdf5_file.root, 'data',\n",
    "                                          tables.Atom.from_dtype(sample_data.dtype),\n",
    "                                          shape=sample_data.shape,\n",
    "                                          filters=filters)\n",
    "    clusters_storage = hdf5_file.create_carray(hdf5_file.root, 'clusters',\n",
    "                                              tables.Atom.from_dtype(sample_clusters.dtype),\n",
    "                                              shape=sample_clusters.shape,\n",
    "                                              filters=filters)\n",
    "    data_storage[:] = sample_data\n",
    "    clusters_storage[:] = sample_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have introduced a few more key PyTables constructs: [compression filters](http://www.pytables.org/usersguide/libref/helper_classes.html#tables.Filters) and [Atom](http://www.pytables.org/usersguide/libref/declarative_classes.html#atomclassdescr) types. \n",
    "\n",
    "Compression filters are incredibly easy to use, and can provide a massive benefit to through put and disk usage. Usage is as simple as what I showed above - simply define a filter and then use it when creating any of the compressible arrays (CArray, EArray (to be introduced)).\n",
    "```\n",
    "filters = tables.Filters(complevel=5, complib='blosc')\n",
    "```\n",
    "Arguments for complib are ``blosc``, ``zlib``, and ``lzo`` , with ``blosc`` being my goto compressor. There is a very in depth discussion of different compression algorithms [here](http://www.pytables.org/usersguide/optimization.html?highlight=optimization#compressionissues). If you are really tuning PyTables and HDF5 for your application, I highly advise you to look there - but in general ``blosc`` is great, and can make your disk I/O effectively faster than streaming raw bytes.\n",
    "\n",
    "Atoms are a special construct of PyTables which allow you to specify dtypes when creating Arrays. There are several methods such as ``tables.Atom.from_dtype`` which can help you set the dtype, or it is easy to manually specify one using one of the [listed types](http://www.pytables.org/usersguide/libref/declarative_classes.html#atomclassdescr) such as ``tables.Atom.Float32()``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that we could have simplified the previous code by passing the data arrays directly into the `create_carray()` method. PyTables could then infer the values of `atom` and `shape` by inspecting the actual data. The resulting code would look like this:\n",
    "\n",
    "```python\n",
    "hdf5_path = \"my_compressed_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='w') as hdf5_file:\n",
    "    filters = tables.Filters(complevel=5, complib='blosc')\n",
    "    hdf5_file.create_carray(hdf5_file.root, 'data', obj=sample_data, filters=filters)\n",
    "    hdf5_file.create_carray(hdf5_file.root, 'clusters', obj=sample_clusters, filters=filters)\n",
    "```\n",
    "\n",
    "Feel free to copy into a code cell to confirm that the result is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_compressed_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='r') as compressed_hdf5_file:\n",
    "    # Here we slice [:] all the data back into memory, then operate on it\n",
    "    uncompressed_hdf5_data = compressed_hdf5_file.root.data[:]\n",
    "    uncompressed_hdf5_clusters = compressed_hdf5_file.root.clusters[:]\n",
    "\n",
    "plot_clusters(uncompressed_hdf5_data, uncompressed_hdf5_clusters, \"CArray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Stop, EArray\n",
    "So far, so good (so what). I can already anticipate questions such as *\"My data won't fit into memory at all... how can I get it into a PyTables array\"*?\n",
    "\n",
    "Fear not, the [`EArray`](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-earray-class) class is here to save the day! This is the workhorse Array type, and the one I use for 99% of my PyTables usage. It is compressible *and* extendable using the `append()` method. Barring a few special exceptions, the EArray can take care of most data storage needs - lets see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?tables.File.create_earray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_extendable_compressed_data.hdf5\"\n",
    "\n",
    "# create a file as before\n",
    "with tables.open_file(hdf5_path, mode='w') as hdf5_file:\n",
    "    # The compression filter is the same\n",
    "    filters = tables.Filters(complevel=5, complib='blosc')\n",
    "    \n",
    "    # Create the data EArray\n",
    "    data_storage = hdf5_file.create_earray(\n",
    "        # Location of the array\n",
    "        where=hdf5_file.root,\n",
    "        # Array name\n",
    "        name='data',\n",
    "        # the data type atom. We specify it since we are creating an empty array\n",
    "        atom=tables.Atom.from_dtype(sample_data.dtype),\n",
    "        # Array shape. Note that the dimension with shape 0 is resizable\n",
    "        shape=(0, sample_data.shape[-1]),\n",
    "        # Compression filters\n",
    "        filters=filters,\n",
    "        # The expected number of rows that will be added. Used to optimise HDF5 storage.\n",
    "        expectedrows=len(sample_data))\n",
    "    \n",
    "    clusters_storage = hdf5_file.create_earray(\n",
    "        where=hdf5_file.root,\n",
    "        name='clusters',\n",
    "        atom=tables.Atom.from_dtype(sample_clusters.dtype),\n",
    "        shape=(0,),\n",
    "        filters=filters,\n",
    "        expectedrows=len(sample_clusters))\n",
    "    \n",
    "    # append the data into the two earrays, one row at a time\n",
    "    for n, (d, c) in enumerate(zip(sample_data, sample_clusters)):\n",
    "        data_storage.append(sample_data[n][None])\n",
    "        clusters_storage.append(sample_clusters[n][None])\n",
    "        \n",
    "    # Note that the data is appended a row at a time for illustration only.\n",
    "    # Ordinarily, you would just pass the entire sequence of data in one call to append:\n",
    "    #data_storage.append(sample_data)\n",
    "    #clusters_storage.append(sample_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding data and (lossless-ly) compressing using EArray, we want to be sure that data retrieval is easy. Plotting the clusters should take care of this, and we do it just as we did for the CArray. In addition, we can also slice subsets of the EArray (e.g. *hdf5_file.root.data[10:100]*) which only brings *that portion* of the data into memory. For dealing with very large datasets this is extremely useful - only the sliced data is ever in memory, and python will get rid of the memory once the slice is not used anymore.\n",
    "\n",
    "You can control memory usage by how much data is sliced out of an EArray, and python will clean up after you are done using that slice. The data stays on disk, so you can fetch it again if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_extendable_compressed_data.hdf5\"\n",
    "\n",
    "with tables.open_file(hdf5_path, mode='r') as extendable_hdf5_file:\n",
    "    extendable_hdf5_data = extendable_hdf5_file.root.data[:]\n",
    "    extendable_hdf5_clusters = extendable_hdf5_file.root.clusters[:]\n",
    "    plot_clusters(extendable_hdf5_file.root.data[10:100], extendable_hdf5_file.root.clusters[10:100], \"EArray subset\")\n",
    "    plot_clusters(extendable_hdf5_data, extendable_hdf5_clusters, \"full EArray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also open existing HDF5 files created with PyTables, and add new data into an EArray. For example, if you were processing daily data from a point-of-sale application an HDF5 file could be used to store new information as it comes in. As a bonus, your sysadmin/DBA people will have a heart attack because you are not using databases/backups... but I digress. HDF5 files are really great for something that is basically a numpy array, but too large to fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_extendable_compressed_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='a') as extendable_hdf5_file:\n",
    "    extendable_hdf5_data = extendable_hdf5_file.root.data\n",
    "    extendable_hdf5_clusters = extendable_hdf5_file.root.clusters\n",
    "    print(\"Length of current data: %i\" % len(extendable_hdf5_data))\n",
    "    print(\"Length of current cluster labels: %i\" % len(extendable_hdf5_clusters))\n",
    "    n_added = 5\n",
    "    print(\"Now adding %i elements to each\" % n_added)\n",
    "    for n, (d, c) in enumerate(zip(sample_data[:n_added], sample_clusters[:n_added])):\n",
    "        extendable_hdf5_data.append(d[None])\n",
    "        extendable_hdf5_clusters.append(c[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reopen the file and confirm that the new data has been appended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tables.open_file(hdf5_path, mode='r') as extendable_hdf5_file:\n",
    "    print(\"Length of current data: %i\" % len(extendable_hdf5_file.root.data))\n",
    "    print(\"Length of current cluster labels: %i\" % len(extendable_hdf5_file.root.clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EArray is incredibly flexible, and allows for easy numpy array-style data. There are some issues with variable length storage which can be solved by something called VLArray, or by fancy methods on top of EArray which I will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragged Data with VLArray\n",
    "There is one more type of array which is very useful for variable length data like speech (utterances) or text (sentences) - the [`VLArray`](http://www.pytables.org/usersguide/libref/homogenous_storage.html#the-vlarray-class). Though it does not support compression of the data (see note in the above documentation) it is still useful in many cases, and is definitely the simplest way to handle variable length data.\n",
    "\n",
    "Let's create a variation on our clustered data using ragged arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_variable_length_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='w') as hdf5_file:\n",
    "    data_storage = hdf5_file.create_vlarray(hdf5_file.root, 'data', atom=tables.Float32Atom(shape=()))\n",
    "    clusters_storage = hdf5_file.create_vlarray(hdf5_file.root, 'clusters', atom=tables.Int32Atom(shape=()))\n",
    "    random_state = np.random.RandomState()\n",
    "    for n in range(1000):\n",
    "        length = int(100 * random_state.randn() ** 2)\n",
    "        data_storage.append(random_state.randn(length,))\n",
    "        clusters_storage.append([length % random_state.randint(1, 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the variable length approach we have done a few things above. For `data_storage`, we created a random length float vector between length 0 and ~2000, in order to simulate variable length data which is distributed approximately exponentially, which is common for things like text (see [Zipf's law](http://www.hermetic.ch/wfca/zipf.htm) for more details). The clusters remained fixed size, but needed to be a sequence, so we wrapped it in the [] braces which made a list out of the single random integer either 1, 2, 3, or 4.\n",
    "This data doesn't really mean anything, but plotting the lengths clearly shows that we can store variable length data in an HDF5 container quite easily. This is very useful for raw text and raw speech, though it has limitations as well - namely that it does not handle multi-dimensional arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_variable_length_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='r') as variable_hdf5_file:\n",
    "    # read the data\n",
    "    variable_hdf5_data = variable_hdf5_file.root.data\n",
    "    \n",
    "    # create a histogram of the row lengths\n",
    "    all_lengths = np.array([len(d) for d in variable_hdf5_data])\n",
    "    plt.hist(all_lengths, color=\"steelblue\")\n",
    "    plt.title(\"Lengths of fake variable length data\")\n",
    "    plt.figure()\n",
    "    \n",
    "    # create a scatter plot of length vs cluster\n",
    "    clusters = variable_hdf5_file.root.clusters\n",
    "    colors = [\"#9b59b6\", \"#3498db\", \"#e74c3c\", \"#2ecc71\"]\n",
    "    for i in np.unique(clusters):\n",
    "        plt.scatter([len(d) for d in variable_hdf5_data[(clusters==i).ravel()]],\n",
    "                    clusters[(clusters==i).ravel()], color=colors[i])\n",
    "    plt.title(\"Length vs. Class\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression Results\n",
    "So far, all the files have been created with 10000 elements. So what difference did compression make on the file size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -lh *.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is ~15% reduction in file size from the `Array` version (my_data.hdf5) to the compressed `CArray` version (my_compressed_data.hdf5). While useful this is not huge. This is because the sample data are largely random, and thus difficult to compress. When enabling compression on your data, it is worth examining whether compression will help or not. In some cases of data with high entropy, compression may give worse performance than no compression. It all depends on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**The rest of this notebook uses some advanced techniques. Feel free to stop here.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced PyTables - the best of both worlds?\n",
    "\n",
    "We have seen `EArray`, which provides an extendable array in one dimension and can also be compressed and decompressed on the fly to improve \"effective\" disk speed and reduce size on the hard disk drive. We have also seen `VLArray`, which allows for storing \"ragged\" data which is variable length in one dimension (dimension 2). Is there a way we can store variables which are variable size *and* retain compression?\n",
    "\n",
    "**Yes** - by overriding `__getitem__` and some other special tricks, we can do a number of interesting things to PyTables arrays (primarily `EArray`) including compressed arbitrary shape data, memory caching, and more. First up, reshaping on the fly to store ragged arrays with multiple dimensions in a `VLArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_nd_variable_length_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='w') as hdf5_file:\n",
    "    data_storage = hdf5_file.create_vlarray(hdf5_file.root, 'data', tables.Float32Atom(shape=()))\n",
    "    data_shapes_storage = hdf5_file.create_earray(hdf5_file.root, 'data_shape', tables.Int32Atom(), shape=(0, 2), expectedrows=1000)\n",
    "    random_state = np.random.RandomState(1999)\n",
    "    for n in range(1000):\n",
    "        shape = (int(100 * random_state.randn() ** 2), random_state.randint(2, 10))\n",
    "        data_storage.append(random_state.randn(*shape).ravel())\n",
    "        data_shapes_storage.append(np.array(shape)[None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about the way numpy actually stores data, we can use the same trick to store n-dimensional variables in a `VLArray`.\n",
    "\n",
    "If you are not familiar with the way numpy handles data internally, it is pretty awesome. Basically, there are 2 parts to a numpy array - the data, and an iterator over that data. The data is stored \"flat\", regardless of the value of `shape`. This means that *any* valid reshape is simply rewriting the iterator, rather than copying all data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/numpy_data_layout.png\", align=\"right\", width=\"50%\", height=\"50%\">\n",
    "Each of the colored cells have the same actual value, but there are several valid \"views\" onto the same data. We can create these views onto the underlying data using ``reshape``.\n",
    "\n",
    "Knowing this, we can apply the same trick to `VLArray`. By storing a flattened version of the numpy array (using `ravel` or `flatten`) *and* the shape associated with that index (which is a usually constant size, in this case 2D) we can reconstruct arbitrary shaped data by overriding `__getitem__`, which is generally a special python method that controls what happens what the bracket operator ([]) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_nd_variable_length_data.hdf5\"\n",
    "\n",
    "# Note we don't use a context manager to open the file, as this file is used across multiple code cells\n",
    "nd_variable_hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "nd_variable_hdf5_data = nd_variable_hdf5_file.root.data\n",
    "nd_variable_hdf5_shape = nd_variable_hdf5_file.root.data_shape\n",
    "\n",
    "old_data_getter = nd_variable_hdf5_file.root.data.__getitem__\n",
    "shape_getter = nd_variable_hdf5_file.root.data_shape.__getitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to open the HDF5 file and get some shorter names for things we are going to use. We also want to \"save\" the old way that data fetching used to work (old_data_getter) to make our new data getter, which does reshaping for you. Next will define a *new* getter which will be used to fetch data and reshape internally. The first argument (self) shows that eventually we will make this a *class method* in order to make the reshape transparent - but first we need to define what behavior we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "def getter(self, key):\n",
    "    if isinstance(key, numbers.Integral) or isinstance(key, np.integer):\n",
    "        start, stop, step = self._process_range(key, key, 1)\n",
    "        if key < 0:\n",
    "            key = start\n",
    "        return old_data_getter(key).reshape(shape_getter(key))\n",
    "    elif isinstance(key, slice):\n",
    "        start, stop, step = self._process_range(key.start, key.stop, key.step)\n",
    "        raise ValueError(\"Variable length - what should we do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check that the getter fetches data from the old getter and reshapes it, and see that the shape matches what we originally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(getter(nd_variable_hdf5_data, -1).shape)\n",
    "print(nd_variable_hdf5_data[-1].shape)\n",
    "print(nd_variable_hdf5_shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch one key - no problem. But what should be done to fetch a slice (chunk) of variable length data? We can't really put them into a 2D array unless we pad all of them to the same length, or pass back a list of arrays approach. The method taken depends on your bias, but I personally prefer a list of arrays approach, as you can always pad later. Given this, let's update the getter functionality and then monkey patch it to the `VLArray` instance to override `__getitem__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getter(self, key):\n",
    "    if isinstance(key, numbers.Integral) or isinstance(key, np.integer):\n",
    "        start, stop, step = self._process_range(key, key, 1)\n",
    "        if key < 0:\n",
    "            key = start\n",
    "        return old_data_getter(key).reshape(shape_getter(key))\n",
    "    elif isinstance(key, slice):\n",
    "        start, stop, step = self._process_range(key.start, key.stop, key.step)\n",
    "        return [old_data_getter(k).reshape(shape_getter(k)) for k in range(start, stop, step)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we have to use a Python `slice()` object to test. Once we attach this getter to the data we will be able to use the familiar slice syntax - but for now just know `slice(start, stop, step)` is equivalent to `[start:stop:step]`. This will return a list of arrays which we can then process using whatever algorithm we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_ragged_arrays = getter(nd_variable_hdf5_data, slice(-5, -1, 1))\n",
    "print([d.shape for d in list_of_ragged_arrays])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this work, we now need to add this functionality to the `VLArray` instance holding our data. It turns out this is pretty tricky (for reasons involving monkeypatching, c-extensions, and other Python business) - the only way I have found is to create a *fake* subclass, then change *all* instances of that subclass (which should be only this data) to fetch data using this special getter function. It is definitely not perfect, but it works.\n",
    "\n",
    "Here it is also important to note that `shape` of this ragged array storage is actually uninformative - `len(data)` is the best way to see the number of samples, but since every example has a different shape you can't really look at `shape` for the data as a whole. This is a somewhat obvious point in hindsight, but it can bite you in your code, so be sure to be aware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _my_VLArray_subclass(tables.VLArray):\n",
    "    pass\n",
    "\n",
    "nd_variable_hdf5_file.root.data.__class__ = _my_VLArray_subclass\n",
    "_my_VLArray_subclass.__getitem__ = getter\n",
    "\n",
    "print(nd_variable_hdf5_data[-1].shape)\n",
    "list_of_ragged_arrays = nd_variable_hdf5_data[-5:-1]\n",
    "print([d.shape for d in list_of_ragged_arrays])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the shapes here are the same as the shapes above - by implementing a custom getter we have invisibly allowed ourselves to store \"multi-ragged\" (for lack of a better term) data in `VLArray`.\n",
    "\n",
    "OK, we have finished with the file now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression and multi-ragged variables using EArray\n",
    "\n",
    "Now that we have figured out how to \"patch\" objects to use custom getters, how can we use this to store variable length data (like text) in a compressible way? It turns out that we can introduce another \"side column\" that lets us do what `VLArray` does, except with `EArray`! Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_earray_variable_length_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='w') as hdf5_file:\n",
    "    filters = tables.Filters(complevel=5, complib='blosc')\n",
    "    data_storage = hdf5_file.create_earray(hdf5_file.root, 'data',\n",
    "                                          tables.Int32Atom(),\n",
    "                                          shape=(0,),\n",
    "                                          filters=filters,\n",
    "                                          # guess that there will mean of 50 numbers per sample\n",
    "                                          expectedrows=50 * 1000)\n",
    "    data_start_and_stop_storage = hdf5_file.create_earray(hdf5_file.root,\n",
    "                                                         'data_start_and_stop',\n",
    "                                                         tables.Int32Atom(),\n",
    "                                                         shape=(0, 2),\n",
    "                                                         filters=filters)\n",
    "    data_shape_storage = hdf5_file.create_earray(hdf5_file.root,\n",
    "                                                'data_shape',\n",
    "                                                tables.Int32Atom(),\n",
    "                                                shape=(0, 2),\n",
    "                                                filters=filters)\n",
    "    random_state = np.random.RandomState(1999)\n",
    "    start = 0\n",
    "    stop = 0\n",
    "    for n in range(1000):\n",
    "        shape = random_state.randint(2, 10, 2)\n",
    "        length = shape[0] * shape[1]\n",
    "        # fake 2D array of ints (pseudo images) \n",
    "        fake_image = random_state.randint(0, 256, length)\n",
    "        for i in range(len(fake_image)):\n",
    "            data_storage.append(fake_image[i][None])\n",
    "        stop = start + length  # Not inclusive!\n",
    "        data_start_and_stop_storage.append(np.array((start, stop))[None])\n",
    "        data_shape_storage.append(np.array((shape))[None])\n",
    "        start = stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created some fake data, we will begin the same procedure - open the file, create a fake getter, and use that fake getter to transform the data into a representation we want. \n",
    "\n",
    "The key difference between this approach and the earlier approach is that we need to have something that works with `EArray` - the most obvious way (once again borrowing concepts from numpy memory layout) is to put the *entire* data into 1 long chunk, then index into that chunk to slice out the flattened samples, then reshaping those based on the true shape, we can get the actual values we desire. By tracking the start and stop of each flattened sample as well as the true shape, we can achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_earray_variable_length_data.hdf5\"\n",
    "earray_variable_hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "earray_variable_hdf5_data = earray_variable_hdf5_file.root.data\n",
    "earray_variable_hdf5_start_and_stop = earray_variable_hdf5_file.root.data_start_and_stop\n",
    "earray_variable_hdf5_shape = earray_variable_hdf5_file.root.data_shape\n",
    "\n",
    "print(\"Length of flattened data (*not* n_samples!) %i\" % len(earray_variable_hdf5_data))\n",
    "print(\"Length of shapes (actual n_samples) %i\" % len(earray_variable_hdf5_shape))\n",
    "print(\"Length of start and stop (actual n_samples) %i\" % len(earray_variable_hdf5_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_data_getter = earray_variable_hdf5_file.root.data.__getitem__\n",
    "shape_getter = earray_variable_hdf5_file.root.data_shape.__getitem__\n",
    "start_and_stop_getter = earray_variable_hdf5_file.root.data_start_and_stop.__getitem__\n",
    "def getter(self, key):\n",
    "    if isinstance(key, numbers.Integral) or isinstance(key, np.integer):\n",
    "        if key < 0:\n",
    "            key = len(earray_variable_hdf5_shape) + key\n",
    "        data_start, data_stop = start_and_stop_getter(key)\n",
    "        shape = shape_getter(key)\n",
    "        return old_data_getter(slice(data_start, data_stop, 1)).reshape(shape)\n",
    "    elif isinstance(key, slice):\n",
    "        start = key.start if key.start is not None else 0\n",
    "        stop = key.stop if key.stop is not None else len(earray_variable_hdf5_shape)\n",
    "        step = key.step if key.step is not None else 1\n",
    "        pos_keys = [len(earray_variable_hdf5_shape) + k if k < 0 else k\n",
    "                    for k in range(start, stop, step)]\n",
    "        starts_and_stops = [start_and_stop_getter(k) for k in pos_keys]\n",
    "        shapes = [shape_getter(k) for k in pos_keys]\n",
    "        return [old_data_getter(slice(dstrt, dstp, 1)).reshape(shp)\n",
    "                for ((dstrt, dstp), shp) in zip(starts_and_stops, shapes)]\n",
    "print(\"Testing single key %s\" % str(getter(earray_variable_hdf5_data, -1).shape))\n",
    "print(\"Testing slice %i\" % len(getter(earray_variable_hdf5_data, slice(-20, -1, 1))))\n",
    "                                                                \n",
    "class _my_EArray_subclass(tables.EArray):\n",
    "    pass\n",
    "earray_variable_hdf5_file.root.data.__class__ = _my_EArray_subclass\n",
    "_my_EArray_subclass.__getitem__ = getter\n",
    "_my_EArray_subclass.__len__ = earray_variable_hdf5_shape.__len__\n",
    "print(earray_variable_hdf5_data[-2].shape)\n",
    "print(len(earray_variable_hdf5_data))\n",
    "print(len(earray_variable_hdf5_data[-20:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_points = sum([shp[0] * shp[1] for shp in earray_variable_hdf5_shape])\n",
    "print(\"Total number of datapoints %i\" % all_points)\n",
    "print(\"kB for raw storage %f\" % ((all_points * 4.) / float(1E3)))\n",
    "!ls -lh my_earray_variable*.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive calculation of this data shows that for uncompressed storage of 29960 int32 values, this is raw storage of (4 bytes per int32 x 29960 int32 points) = 119.840 kB. But looking at the actual storage, we use less than 50% of that *even* with the extra data needed for start, stop and shape. This 50% gain in storage *also* equates to effectively ~50% faster reads from disk - awesome! It's like buying new hardware without spending any money.\n",
    "\n",
    "The only downside is that decompressing on the fly will use *more* CPU than reading the data from disk raw - but in general this overhead is much less than the gain you get from reading data from disk 50% faster. In general we have much more CPU available than excess disk read throughput, though this starts changing if you have an SSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "earray_variable_hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One More Trick\n",
    "\n",
    "There is one more area we can optimize - memory usage. By reading more data into memory at a time, we can improve processing times by hitting the disk less often, and reading longer continuous reads. For spinning disk, this is the best way to read a file - since it is spinning in circles you are just reading the bits in order rather than having to spin the disk back and forth.\n",
    "\n",
    "HDF5/PyTables support some of this with in-memory processing, but the rest can be done using getter override tricks like we have seen above. First, let's see how we can read from an [in-memory HDF5 file](http://www.pytables.org/cookbook/inmemory_hdf5_files.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sample data\n",
    "sample_data, sample_clusters = make_random_cluster_points(10000)\n",
    "hdf5_path = \"my_inmemory_data.hdf5\"\n",
    "\n",
    "# write the file\n",
    "with tables.open_file(hdf5_path, mode='w') as hdf5_file:\n",
    "    data_storage = hdf5_file.create_array(hdf5_file.root, 'data', sample_data)\n",
    "    clusters_storage = hdf5_file.create_array(hdf5_file.root, 'clusters', sample_clusters)\n",
    "    \n",
    "# Open as an in-memory file using the H5FD_CORE driver\n",
    "with tables.open_file(hdf5_path, mode='r', driver='H5FD_CORE') as read_hdf5_file:\n",
    "    hdf5_data = read_hdf5_file.root.data[:]\n",
    "    hdf5_clusters = read_hdf5_file.root.clusters[:]\n",
    "\n",
    "plot_clusters(hdf5_data, hdf5_clusters, \"in memory PyTables array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would also like to be able to trade off memory usage for access speed for something in between `H5FD_CORE` and slices from the dataset. For sequential access (like iterating minibatches for KMeans, neural networks, etc.) this can be achieved using some special getter tricks. One example of this can be seen in my library [dagbldr](https://github.com/dagbldr/dagbldr/blob/master/dagbldr/datasets/dataset_utils.py#L5). I will post a function from that library here for completeness, but in general the latest version (and other tricks) will be found in my library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_memory_swapper(earray, mem_size):\n",
    "    class _cEArray(tables.EArray):\n",
    "        pass\n",
    "\n",
    "    # Filthy hack to override getter which is a cextension...\n",
    "    earray.__class__ = _cEArray\n",
    "\n",
    "    earray._in_mem_size = int(float(mem_size))\n",
    "    assert earray._in_mem_size >= 1E6 # anything smaller than 1MB is pretty worthless\n",
    "    earray._in_mem_slice = np.empty([1] * len(earray.shape)).astype(\"float32\")\n",
    "    earray._in_mem_limits = [np.inf, -np.inf]\n",
    "\n",
    "    old_getter = earray.__getitem__\n",
    "\n",
    "    def _check_in_mem(earray, start, stop):\n",
    "        lower = earray._in_mem_limits[0]\n",
    "        upper = earray._in_mem_limits[1]\n",
    "        if start < lower or stop > upper:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def _load_in_mem(earray, start, stop):\n",
    "        # start and stop are slice indices desired - we calculate different\n",
    "        # sizes to put in memory\n",
    "        n_bytes_per_entry = earray._in_mem_slice.dtype.itemsize\n",
    "        n_entries = earray._in_mem_size / float(n_bytes_per_entry)\n",
    "        n_samples = earray.shape[0]\n",
    "        n_other = earray.shape[1:]\n",
    "        n_samples_that_fit = int(n_entries / np.prod(n_other))\n",
    "        assert n_samples_that_fit > 0\n",
    "        # handle - index case later\n",
    "        assert start >= 0\n",
    "        assert stop >= 0\n",
    "        assert stop >= start\n",
    "        slice_size = stop - start\n",
    "        if slice_size > n_samples_that_fit:\n",
    "            err_str = \"Slice from [%i:%i] (size %i) too large! \" % (start, stop, slice_size)\n",
    "            err_str += \"Max slice size %i\" % n_samples_that_fit\n",
    "            raise ValueError(err_str)\n",
    "        slice_limit = [start, stop]\n",
    "        earray._in_mem_limits = slice_limit\n",
    "        if earray._in_mem_slice.shape[0] == 1:\n",
    "            # allocate memory\n",
    "            print(\"Allocating %i bytes of memory for EArray swap buffer\" % earray._in_mem_size)\n",
    "            earray._in_mem_slice = np.empty((n_samples_that_fit,) + n_other, dtype=earray.dtype)\n",
    "        # handle edge case when last chunk is smaller than what slice will\n",
    "        # return\n",
    "        limit = min([slice_limit[1] - slice_limit[0], n_samples - slice_limit[0]])\n",
    "        earray._in_mem_slice[:limit] = old_getter(\n",
    "            slice(slice_limit[0], slice_limit[1], 1))\n",
    "\n",
    "    def getter(self, key):\n",
    "        if isinstance(key, numbers.Integral) or isinstance(key, np.integer):\n",
    "            start, stop, step = self._process_range(key, key, 1)\n",
    "            if key < 0:\n",
    "                key = start\n",
    "            if _check_in_mem(self, key, key):\n",
    "                lower = self._in_mem_limits[0]\n",
    "            else:\n",
    "                # slice into memory...\n",
    "                _load_in_mem(self, key, key)\n",
    "                lower = self._in_mem_limits[0]\n",
    "            return self._in_mem_slice[key - lower]\n",
    "        elif isinstance(key, slice):\n",
    "            start, stop, step = self._process_range(key.start, key.stop, key.step)\n",
    "            if _check_in_mem(self, start, stop):\n",
    "                lower = self._in_mem_limits[0]\n",
    "            else:\n",
    "                # slice into memory...\n",
    "                _load_in_mem(self, start, stop)\n",
    "                lower = self._in_mem_limits[0]\n",
    "            return self._in_mem_slice[start - lower:stop - lower:step]\n",
    "    # This line is critical...\n",
    "    _cEArray.__getitem__ = getter\n",
    "    return earray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining this function, we should be able to add a memory swapper onto an `EArray` and transparently control the amount of CPU memory we use to speed up processing - giving us a tradeoff between memory usage and prcessing speed which is one of the most common trade-offs made in programming. Depending on your hardware, anywhere from \"only a slice at a time\" as in regular HDF5, to \"all in memory\" as by having a large value in `add_memory_swapper` or using the `H5FD_CORE` driver when opening the file is available to trade through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_memory_extendable_compressed_data.hdf5\"\n",
    "with tables.open_file(hdf5_path, mode='w') as hdf5_file:\n",
    "    filters = tables.Filters(complevel=5, complib='blosc')\n",
    "    data_storage = hdf5_file.create_earray(hdf5_file.root, 'data',\n",
    "                                          tables.Atom.from_dtype(sample_data.dtype),\n",
    "                                          shape=(0, sample_data.shape[-1]),\n",
    "                                          filters=filters,\n",
    "                                          expectedrows=len(sample_data))\n",
    "    clusters_storage = hdf5_file.create_earray(hdf5_file.root, 'clusters',\n",
    "                                              tables.Atom.from_dtype(sample_clusters.dtype),\n",
    "                                              shape=(0,),\n",
    "                                              filters=filters,\n",
    "                                              expectedrows=len(sample_clusters))\n",
    "    for n, (d, c) in enumerate(zip(sample_data, sample_clusters)):\n",
    "        data_storage.append(sample_data[n][None])\n",
    "        clusters_storage.append(sample_clusters[n][None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"my_memory_extendable_compressed_data.hdf5\"\n",
    "memory_extendable_hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "memory_extendable_hdf5_data = add_memory_swapper(memory_extendable_hdf5_file.root.data, 10E6)\n",
    "memory_extendable_hdf5_clusters = memory_extendable_hdf5_file.root.clusters\n",
    "plot_clusters(memory_extendable_hdf5_file.root.data[10:100],\n",
    "              memory_extendable_hdf5_file.root.clusters[10:100], \"EArray subset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the data, we can look at the memory limits used internally by the memory swapper. After taking different slices we see the `_in_mem_limits` move - this means the swapper is reading new data from disk into memory. However, taking a slice which is already in memory will not invoke another copy from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Current memory limits %s\" % str(memory_extendable_hdf5_data._in_mem_limits))\n",
    "memory_extendable_hdf5_data[-100:]\n",
    "print(\"Moved memory limits %s\" % str(memory_extendable_hdf5_data._in_mem_limits))\n",
    "memory_extendable_hdf5_data[-5:]\n",
    "print(\"Unchanged memory limits %s\" % str(memory_extendable_hdf5_data._in_mem_limits))\n",
    "memory_extendable_hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using HDF5 with PyTables makes almost any operation possible, even on low memory hardware. By playing some advanced tricks with `__getitem__` it is possible to create containers which compress lots of data into small storage, helping us process from disk faster giving more effective throughput for a system. In addition, we can also use these same tricks to trade memory for speed for uniform 2D arrays, with easy extension to arbitrary nd-array processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
