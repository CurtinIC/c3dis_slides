{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The Mandelbrot set is the set of complex numbers $c$ for which the function $f_{c}(z)=z^{2}+c$ does not diverge when iterated from $z=0$.\n",
    "\n",
    "Images of the Mandelbrot set are created by sampling a range of complex numbers and for each point determining how the preceeding iterative function behaves. The examples in this notebook use the [Escape time algorithm](https://en.wikipedia.org/wiki/Mandelbrot_set#Escape_time_algorithm), where the colour assigned to each image pixel is determined by how many iterations are required for the function to diverge or not. The algorithm performs an early exit as soon as the function value exceeds a given threshold or horizon. Pixels that do not diverge after the maximum number of iterations are assumed to belong to the Mandelbrot set and are assigned the colour black. Ironically, this means that the interesting images commonly associated with the Mandelbrot set are actually plots of all the points that *don't* belong to the set!\n",
    "\n",
    "## About the Computational Characteristics\n",
    "Calculating the members of the Mandelbrot Set is an example of an [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) problem. In this case, each pixel of the image can be calculated without reference to either the inputs or results of any other pixel. The computation is loop-heavy, and does not require any disc IO. Performance tends to be dominated by CPU speed, algorithm details (particularly in the inner loop), and memory access patterns (cache performance). This may be classed as a typical [CPU-bound](https://en.wikipedia.org/wiki/CPU-bound) problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics covered\n",
    "* Python type hinting\n",
    "* [Google Style](https://google.github.io/styleguide/pyguide.html) pydoc strings\n",
    "* Passing functions as arguments to other functions\n",
    "* Plotting with matplotlib\n",
    "* Just-in-time compilation with numba\n",
    "* Using Cython\n",
    "* Vectorisation, and the importance of cache-friendly code\n",
    "* Profiling inside Jupyter Notebooks\n",
    "    * `%timeit` to measure average execution time\n",
    "    * `%prun`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numexpr as ne\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from numba import jit\n",
    "%matplotlib inline\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Version\n",
    "First, define the function that determines Mandelbrot set membership for a complex number using the Escape time algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mandelbrot_simple(\n",
    "    z: complex, \n",
    "    horizon: float, \n",
    "    max_iterations: int) -> int:\n",
    "    \"\"\" Calculate Mandelbrot set membership for a given complex number.\n",
    "    \n",
    "    Once the iterative function value exceeds the horizon, the algorithm escapes through an early exit, \n",
    "    returning the current number of iterations. If the value does not exceed the horizon by in the maximum \n",
    "    number of iterations, then the point is assumed to be a member of the Mandelbrot set, \n",
    "    and the special flag value of 0 is returned.\n",
    "    \n",
    "    Args:\n",
    "        z (complex): The complex number to calculate\n",
    "        horizon (float): The escape horizon.\n",
    "        max_iterations (int): Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        The number of iterations required for divergence, or 0 if the max number of iterations was reached.\n",
    "    \"\"\"\n",
    "    c = z\n",
    "    z = 0.0j\n",
    "    for n in range(max_iterations):\n",
    "        # Does the magnitude of z exceed the horizon?\n",
    "        if np.abs(z) > horizon:\n",
    "            # Yes, so escape and return the number of iterations so far\n",
    "            return n\n",
    "        # No, so perform another iteration\n",
    "        z = z * z + c\n",
    "        \n",
    "    # Did not diverge after max_iterations\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a function that loops over the sampled complex numbers, calling the mandelbrot_simple function for each number in a 2D array. \n",
    "\n",
    "Our first approach therefore uses nested loops to traverse the image pixels in row-major order.\n",
    "\n",
    "Another feature to note is that the iterative function called at each pixel is passed as an argument. This will allow us to easily reuse this function with different versions of the per-pixel function.\n",
    "\n",
    "First, let's define some global constants that will allow us to quickly change the defaults used in all implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_r_min = -2.0\n",
    "default_r_max = 0.5\n",
    "default_i_min = -1.25\n",
    "default_i_max = 1.25\n",
    "default_r_samples = 800\n",
    "default_i_samples = 800\n",
    "default_horizon = 20000.0\n",
    "default_max_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mandelbrot_set(\n",
    "    mandelbrot_func: callable = mandelbrot_simple,\n",
    "    r_min: float = default_r_min,\n",
    "    r_max: float = default_r_max,\n",
    "    i_min: float = default_i_min,\n",
    "    i_max: float = default_i_max,\n",
    "    r_samples: int = default_r_samples,\n",
    "    i_samples: int = default_i_samples,\n",
    "    horizon: float = default_horizon,\n",
    "    max_iterations: int = default_max_iterations):\n",
    "    \"\"\" Mandelbrot set membership, version 1.\n",
    "    \n",
    "    Samples points in the complex plane and for each point calculates whether the \n",
    "    point is a member of the Mandelbrot set.\n",
    "    \n",
    "    Args:\n",
    "        mandelbrot_func (callable): The function to call at each sampled point.\n",
    "        r_min (float): lower bound of the real number interval\n",
    "        r_max (float): upper bound of the real number interval\n",
    "        i_min (float): lower bound of the imaginary number interval\n",
    "        i_max (float): upper bound of the imaginary number interval\n",
    "        r_samples (int): Number of samples along the real axis\n",
    "        i_samples (int): Number of samples along the imaginary axis\n",
    "        horizon (float): The escape horizon.\n",
    "        max_iterations (int): Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        A 3-tuple containing the real parts of the sampled complex numbers, \n",
    "        the imaginary parts, and the number of iterations before divergence, \n",
    "        all in numpy ndarrays.\n",
    "    \"\"\"\n",
    "    # Create a linearly-spaced sample of single-precision points in the complex plane\n",
    "    real_parts = np.linspace(r_min, r_max, r_samples, dtype=np.float32)\n",
    "    imaginary_parts = np.linspace(i_min, i_max, i_samples, dtype=np.float32)\n",
    "    \n",
    "    # We need somewhere to store the number of iterations at each point. \n",
    "    # Lets use a r_samples * i_samples ndarray\n",
    "    num_iterations = np.zeros((r_samples, i_samples))\n",
    "    \n",
    "    # What happens if the imaginary_parts[j]*1j is moved into a ufunc operation outside the loop?\n",
    "    for i in range(r_samples):\n",
    "        for j in range(i_samples):\n",
    "            num_iterations[i, j] = mandelbrot_func(\n",
    "                real_parts[i] + 1j * imaginary_parts[j],\n",
    "                horizon,\n",
    "                max_iterations)\n",
    "            \n",
    "    return (real_parts, imaginary_parts, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to examining the performance, let's make a pretty picture..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering the Mandelbrot Set\n",
    "Although the focus of this notebook is on performance characteristics of the Mandelbrot set generation, it would be a shame to not make some pretty fractal images. To do so, we turn to matplotlib, defining a function that can render an inline image from our results tuples. In addition, it displays a colorbar indicating the number of iterations mapped to the colours.\n",
    "\n",
    "The gamma correction is used to account for the fact that most pixels diverge after just a few iterations, and thus all the colours are bunched into one part of the colour map. You can see the effect of gamma correction in the colour bars by adjusting the gamma value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def render(\n",
    "    results, \n",
    "    image_width=10,\n",
    "    image_height=10,\n",
    "    r_min=default_r_min,\n",
    "    r_max=default_r_max,\n",
    "    i_min=default_i_min,\n",
    "    i_max=default_i_max,\n",
    "    dpi=72, \n",
    "    cmap='inferno', \n",
    "    gamma=0.3):\n",
    "    \n",
    "    # unpack the results tuple into x, y, and z.\n",
    "    # (x, y) are the image coordinates.\n",
    "    # z is the function value to be mapped to a colour value\n",
    "    x,y,z = results\n",
    "    \n",
    "    num_pixels_x = len(x)\n",
    "    num_pixels_y = len(y)\n",
    "    \n",
    "    # create the matplotlib figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(image_width, image_height), dpi=dpi)\n",
    "    x_tick_locations = np.arange(0, num_pixels_x, 3 * dpi)\n",
    "    x_tick_labels = r_min + (r_max - r_min) * x_tick_locations / num_pixels_x\n",
    "    plt.xticks(x_tick_locations, x_tick_labels)\n",
    "    \n",
    "    y_tick_locations = np.arange(0, num_pixels_y, 3 * dpi)\n",
    "    y_tick_labels = i_min + (i_max - i_min) * y_tick_locations / num_pixels_y\n",
    "    plt.yticks(y_tick_locations, y_tick_labels)\n",
    "          \n",
    "    # Calculate the power norm used for colour correction\n",
    "    norm = colors.PowerNorm(gamma)\n",
    "    plt.imshow(z.T, cmap=cmap, origin='lower', norm=norm, interpolation='bicubic')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a render function, we can calculate an image. We first determine the number of complex points to sample, starting from the desired image dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "dpi = 72\n",
    "num_pixels_x = dpi * width\n",
    "num_pixels_y = dpi * height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the set memberships. Since the simple implementation is quite slow, we override the default `max_iterations` with a smaller value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = mandelbrot_set(\n",
    "    r_samples=num_pixels_x,\n",
    "    i_samples=num_pixels_y,\n",
    "    max_iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(results, image_width=width, image_height=height, dpi=dpi, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to explore the visual world of the Mandelbrot set, I suggest waiting until the end of this notebook when a much faster method of calculation is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking our Results\n",
    "To help visualise the different performance improvements, let's define a class to track and plot our timing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Results(object):\n",
    "    def __init__(self, baseline_time, baseline_label, color):\n",
    "        self.speedups = [1]\n",
    "        self.labels = [baseline_label]\n",
    "        self.colors = [color]\n",
    "        self.baseline_time = baseline_time\n",
    "        self.times = []\n",
    "        \n",
    "         # ugly code to handle Jupyter version differences\n",
    "        try:\n",
    "            # newer versions of TimeItResult contain the average time already\n",
    "            self.times.append(self.baseline_time.average)\n",
    "        except:\n",
    "            # Failing that, older versions have a list of all times\n",
    "            self.times.append(np.mean(self.baseline_time.all_runs))\n",
    "        \n",
    "    def speedup_plot(self, log_scale=False):\n",
    "        ind = np.arange(len(self.speedups))\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        \n",
    "        for name, index, speedup, color in zip(self.labels, ind, self.speedups, self.colors):\n",
    "            ax.bar(\n",
    "                index,\n",
    "                speedup, \n",
    "                width=0.5, \n",
    "                color=color,\n",
    "                label=name,\n",
    "                log=log_scale)\n",
    "            \n",
    "        ax.set_xticks(ind)\n",
    "        ax.set_xticklabels(self.labels)\n",
    "        ax.set_title('Speedup relative to {}'.format(self.labels[0]))\n",
    "        ax.legend(self.labels)\n",
    "        plt.show()\n",
    "        return self\n",
    "        \n",
    "    def walltime_plot(self, log_scale=False):\n",
    "        ind = np.arange(len(self.speedups))\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        \n",
    "        for name, index, walltime, color in zip(self.labels, ind, self.times, self.colors):\n",
    "            ax.bar(\n",
    "                index,\n",
    "                walltime, \n",
    "                width=0.5, \n",
    "                color=color,\n",
    "                label=name,\n",
    "                log=log_scale)\n",
    "            \n",
    "        ax.set_xticks(ind)\n",
    "        ax.set_xticklabels(self.labels)\n",
    "        ax.set_title('Walltime')\n",
    "        ax.legend(self.labels)\n",
    "        plt.show()\n",
    "        return self\n",
    "        \n",
    "    def add_time(self, time, label, color):\n",
    "        \n",
    "        baseline_avg = self.times[0]\n",
    "        # ugly code to handle Jupyter version differences\n",
    "        try:\n",
    "            # newer versions of TimeItResult contain the average time already\n",
    "            self.speedups.append(baseline_avg / time.average)\n",
    "            self.times.append(time.average)\n",
    "        except:\n",
    "            # Failing that, older versions have a list of all times\n",
    "            avg = np.mean(time.all_runs)\n",
    "            self.speedups.append(baseline_avg / avg)\n",
    "            self.times.append(avg)\n",
    "            \n",
    "        self.labels.append(label)\n",
    "        self.colors.append(color)\n",
    "        \n",
    "        # return a reference to self to allow chaining\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Performance of mandelbrot_set_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we will use the default parameters from now on. First, lets measure the execution time of `mandelbrot_set_simple`. Note that our simple implementation is so slow that we deliberately limit the number of calls to our function (the `%timeit` default is to make multiple calls and then average the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_simple = %timeit -n 1 -r 1 -o results = mandelbrot_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a results object and add our baseline result. This will be used to track our results and calculate the relative speedup. We don't bother with a plot until after gathering the second result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = Results(times_simple, 'simple', 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "We are often advised to profile our code first before making optimisations. So lets try the `%prun` magic that reports function-level profiling information. To make this a bit faster, the number of samples is reduced, but the results should still be representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun mandelbrot_set(r_samples=300, i_samples=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clear majority of time is spent in the per-pixel `mandelbrot_simple` function, so lets see if we can improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the `mandelbrot_simple` function. The divergence check compares the magnitude (calculated with `np.abs`) of the current value of `z` with the `horizon` parameter. This is defined as $\\lvert x + iy \\rvert = \\sqrt{x^2 + y^2}$\n",
    "\n",
    "Square roots can be expensive..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 3 + 4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time np.abs(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't take long, but is there a faster way? One common method to avoid a square root when making a magnitude comparison is to compare the squared magnitude. \n",
    "\n",
    "How long does calculating the squared magnitude of a complex number take? The squared magnitude for $x + iy$ is $x^2 + y^2$. We don't have a built-in operator for this function, so it is written in full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time n.real * n.real + n.imag * n.imag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my computer, that takes approximately 1/5 of the time. Let's see what difference that saving makes when buried in the inner loop that is executed thousands of times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mandelbrot_magsquared(z, horizon, max_iterations):\n",
    "    c = z\n",
    "    z = 0.0j\n",
    "    \n",
    "    # The horizon is a loop-invariant, therefore it can be calculated just once\n",
    "    horizon_squared = horizon * horizon\n",
    "    \n",
    "    for n in range(max_iterations):\n",
    "        # Does the magnitude of z exceed the horizon?\n",
    "        if z.real * z.real + z.imag * z.imag > horizon_squared:\n",
    "            # Yes, so escape and return the number of iterations so far\n",
    "            return n\n",
    "        # No, so perform another iteration\n",
    "        z = z * z + c\n",
    "        \n",
    "    # Did not diverge after max_iterations\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_magsquared = %timeit -n 1 -r 1 -o mandelbrot_set(mandelbrot_func=mandelbrot_magsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results.add_time(times_magsquared, 'mag squared', 'skyblue').speedup_plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little faster.\n",
    "\n",
    "Notice how the `z.real * z.real + z.imag * z.imag` calculation is similar to the expression that squares z a few lines down? What if we stop using the `complex` type, and handle the real and imaginary parts separately all the way through? This might allow doing some calculations only once and then reusing the results, thus reducing number of calculations for each iteration. Note that the square of a complex number is defined as:\n",
    "\n",
    "$$(x + iy)^2 = x^2 + 2xiy + (iy)^2$$\n",
    "\n",
    "Splitting into separate calculations for the real and imaginary parts, and noting that $i^2 = -1$ gives\n",
    "\n",
    "$$real = x^2 - y^2$$ \n",
    "\n",
    "$$imag = 2xy$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mandelbrot_complex_number_split(z, horizon, max_iterations):\n",
    "    c_real = z.real\n",
    "    c_imag = z.imag\n",
    "    z_real = 0.0\n",
    "    z_imag = 0.0\n",
    "    \n",
    "    # The horizon is a loop-invariant, therefore it can be calculated just once\n",
    "    horizon_squared = horizon * horizon\n",
    "    \n",
    "    for n in range(max_iterations):\n",
    "        # precompute some shared values\n",
    "        z_real_squared = z_real * z_real\n",
    "        z_imag_squared = z_imag * z_imag\n",
    "        # Does the magnitude of z exceed the horizon?\n",
    "        if z_real_squared + z_imag_squared > horizon_squared:\n",
    "            # Yes, so escape and return the number of iterations so far\n",
    "            return n\n",
    "        # No, so perform another iteration\n",
    "        # Note that we calculate the new value of z_imag first. Why?\n",
    "        z_imag = 2 * z_real * z_imag + c_imag\n",
    "        z_real = z_real_squared - z_imag_squared + c_real\n",
    "                \n",
    "    # Did not diverge after max_iterations\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_complex_number_split = %timeit -n 1 -r 1 -o mandelbrot_set(mandelbrot_func=mandelbrot_complex_number_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.add_time(times_complex_number_split, 'complex split', 'cadetblue').speedup_plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little faster again, but the code is getting harder to read. And we are still only getting less than twice the speedup of the original. Is that the best we can hope for with Python? Should we be reaching for C or Fortran? Before going that far, let's look at some other options that stay with Python.\n",
    "\n",
    "There is actually a serious performance problem with the simple implementation that cannot be solved by optimising the per-pixel function. Can you see what it is?\n",
    "\n",
    "**Hint:** It's related to loops and data access. Let's look at a NumPy implementation to see if it can shed some light on what is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy\n",
    "An internet search reveals lots of ways to rewrite this algorithm for numpy. Here we will look at just one approach while comparing the square-root removal optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mandelbrot_numpy_vectorised_1(c, horizon, max_iterations):\n",
    "    'Vectorised Numpy implementation with magnitude escape test'\n",
    "    iterations = np.zeros(c.shape)\n",
    "    z = np.zeros(c.shape, np.complex64)\n",
    "    \n",
    "    for it in range(max_iterations):\n",
    "        # check for divergence using the original magnitude test\n",
    "        notdone = np.less(np.absolute(z), horizon)\n",
    "        iterations[notdone] = it\n",
    "        z[notdone] = z[notdone]**2 + c[notdone]\n",
    "    iterations[iterations == max_iterations - 1] = 0\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mandelbrot_numpy_vectorised_2(c, horizon, max_iterations):\n",
    "    'Vectorised Numpy implementation with magnitude-squared escape test'\n",
    "    iterations = np.zeros(c.shape)\n",
    "    z = np.zeros(c.shape, np.complex64)\n",
    "    \n",
    "    # The horizon is a loop-invariant, therefore it can be calculated just once\n",
    "    horizon_squared = horizon * horizon\n",
    "    \n",
    "    for it in range(max_iterations):\n",
    "        # Note that we are incorporating the magnitude-squared optimisation to avoid a square root\n",
    "        notdone = np.less(z.real*z.real + z.imag*z.imag, horizon_squared)\n",
    "        iterations[notdone] = it\n",
    "        z[notdone] = z[notdone]**2 + c[notdone]\n",
    "    iterations[iterations == max_iterations - 1] = 0\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few interesting points to note about this implementation, including:\n",
    "* the use of boolean indexing to stop calculations on pixels that have already diverged,\n",
    "* rather than calculating pixels one at a time, all pixels have a single iteration performed at the same time\n",
    "\n",
    "The `mandelbrot_numpy_vectorised` function takes an entire numpy array of complex numbers instead of a single complex number. This requires a new `mandelbrot_set` wrapper function to call the vectorised calculation functions. We didn't call this wrapper `mandelbrot_set_numpy` as we reuse it with other implementations later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mandelbrot_set_vectorised(\n",
    "    mandelbrot_func: callable,\n",
    "    r_min: float = default_r_min,\n",
    "    r_max: float = default_r_max,\n",
    "    i_min: float = default_i_min,\n",
    "    i_max: float = default_i_max,\n",
    "    r_samples: int = default_r_samples,\n",
    "    i_samples: int = default_i_samples,\n",
    "    horizon: float = default_horizon,\n",
    "    max_iterations: int = default_max_iterations):\n",
    "    \n",
    "    # Create a linearly-spaced sample of single-precision points in the complex plane\n",
    "    real_parts = np.linspace(r_min, r_max, r_samples, dtype=np.float32)\n",
    "    imaginary_parts = np.linspace(i_min, i_max, i_samples, dtype=np.float32)\n",
    "    # Combine the real and imaginary samples into a single 2D array of complex numbers\n",
    "    c = real_parts + imaginary_parts[:,None]*1j\n",
    "        \n",
    "    # calculate set membership\n",
    "    num_iterations = mandelbrot_func(c, horizon, max_iterations)\n",
    "            \n",
    "    # Transpose the num_iterations array so the shape and pixel ordering matches the previous implementations\n",
    "    return (real_parts, imaginary_parts, num_iterations.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "times_numpy_1 = %timeit -o mandelbrot_set_vectorised(mandelbrot_func=mandelbrot_numpy_vectorised_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_numpy_2 = %timeit -o mandelbrot_set_vectorised(mandelbrot_func=mandelbrot_numpy_vectorised_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.add_time(times_numpy_1, 'numpy', 'firebrick')\n",
    "results.add_time(times_numpy_2, 'numpy mag squared', 'tomato')\n",
    "results.speedup_plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a much more impressive speedup than our optimisations to the `mandelbrot_simple` function. We can also see that the magnitude squared escape test is still worthwhile.\n",
    "\n",
    "What is going on here? How is such a performance increase being achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Access, Cache Misses, and Vectorisation\n",
    "The simple solution uses numpy arrays to hold the data, but fails to take advantage of the memory layout of that data. Instead, the real and imaginary values for each number are copied one at a time, used to construct a new complex number object which is then passed to the per-pixel function. Copying data when creating the new `Complex` object puts that data into a new memory location, meaning it is probably not already in cache. This causes a cache miss when the data is read, leading to a potential CPU stall while data is loaded from system RAM. On the other hand, the NumPy version is implemented with Universal Functions or ufuncs. These have several charactistics that help performance:\n",
    "* implemented to take advantage of the contiguous memory layout of Numpy ndarrays, leading to substantially more effective use of cache memory.\n",
    "* frequently implemented in C, bypassing some additional Python overheads such as bounds checking\n",
    "* operate directly on the data, avoiding the additional copies of our simple implementation and often avoiding index safety checks\n",
    "* depending on numpy compile options, they may call into optimised linear algebra libraries like BLAS or Intel MKL for some functions\n",
    "\n",
    "Since accessing higher cache levels or main RAM is substantially slower than most CPU instructions, cache miss effects can easily dominate performance in loop-heavy CPU-bound code like this function we are exploring here.\n",
    "\n",
    "Unfortunately, the simple profiling tools (such as `%timeit` and `%prun`) available in notebooks do not allow cache performance profiling.\n",
    "\n",
    "NumPy is not the end of the story though. NumExpr is another library that can improve performance over NumPy for some data sets and algorithms. Let's examine it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numexpr\n",
    "\n",
    "[Numexpr](https://github.com/pydata/numexpr) is a fast numerical expression evaluator for NumPy. It avoids allocating memory for intermediate results, leading to improved cache utilisation when compared to the default NumPy implementations.\n",
    "\n",
    "However, in practice, Numexpr sometimes performs better and sometimes worse than the corresponding NumPy implementation. According to the developers, it does best on large matrices that don't fit into CPU cache. You can explore the impact of matrix size on the relative performance of Numexpr by adjusting the values for `default_r_samples` and `default_i_samples` before running all cells again. **Note:** if you increase the array dimensions to more than 1000, you probably need to lower `max_iterations` to a small number like 50, otherwise the total execution time will be too high.\n",
    "\n",
    "Fortunately we require only minor modifications to the NumPy implementation to use Numexpr, so let's see how it does. We only examine the magnitude-squared escape test here. If you want to compare the original magnitude test, try modifying the code yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mandelbrot_numexpr(c, horizon, max_iterations):\n",
    "    iterations = np.zeros(c.shape)\n",
    "    z = np.zeros(c.shape, np.complex64)\n",
    "    \n",
    "    # The horizon is a loop-invariant, therefore it can be calculated just once\n",
    "    horizon_squared = horizon * horizon\n",
    "    \n",
    "    for it in range(max_iterations):\n",
    "        notdone = ne.evaluate('z.real*z.real + z.imag*z.imag < horizon_squared')\n",
    "        iterations[notdone] = it\n",
    "        z = ne.evaluate('where(notdone, z**2 + c, z)')\n",
    "    iterations[iterations == max_iterations - 1] = 0\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_numexpr = %timeit -o mandelbrot_set_vectorised(mandelbrot_func=mandelbrot_numexpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.add_time(times_numexpr, 'numexpr', 'orange').speedup_plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba\n",
    "So far we are doing really well. On my test machine, the Numexpr version is giving ~75x speedup.\n",
    "\n",
    "But we are not out of options yet. [Numba](http://numba.pydata.org/) is a new library that uses [LLVM](http://llvm.org/) to perform just-in-time (JIT) compilation to native code. It is particularly well suited to array computations. It integrates well with the SciPy stack and can target both CPU and GPU hardware. Let's see how easy it is to compile our Mandelbrot functions. Essentially all we need to do in this case is add the `@jit` decorator to our original simple implementation. Yes, the one with the slow loops and poor memory access patterns!\n",
    "\n",
    "Note that the per-pixel function is hard-coded. Numba will work if this function is passed as an argument but performance degrades drastically (try it and see). Why do you think this happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def mandelbrot_numba(z, horizon, max_iterations):\n",
    "    c = z\n",
    "    z = 0.0j\n",
    "    for n in range(max_iterations):\n",
    "        # Does the magnitude of z exceed the horizon?\n",
    "        if abs(z) > horizon:\n",
    "            # Yes, so escape and return the number of iterations so far\n",
    "            return n\n",
    "        # No, so perform another iteration\n",
    "        z = z * z + c\n",
    "        \n",
    "    # Did not diverge after max_iterations\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def mandelbrot_set_numba(\n",
    "    r_min: float = default_r_min,\n",
    "    r_max: float = default_r_max,\n",
    "    i_min: float = default_i_min,\n",
    "    i_max: float = default_i_max,\n",
    "    r_samples: int = default_r_samples,\n",
    "    i_samples: int = default_i_samples,\n",
    "    horizon: float = default_horizon,\n",
    "    max_iterations: int = default_max_iterations):\n",
    "    \n",
    "    # Create a linearly-spaced sample of single-precision points in the complex plane\n",
    "    real_parts = np.linspace(r_min, r_max, r_samples, dtype=np.float32)\n",
    "    imaginary_parts = np.linspace(i_min, i_max, i_samples, dtype=np.float32)\n",
    "    num_iterations = np.zeros((r_samples, i_samples))\n",
    "    \n",
    "    for i in range(r_samples):\n",
    "        for j in range(i_samples):\n",
    "            num_iterations[i, j] = mandelbrot_numba(\n",
    "                real_parts[i] + 1j * imaginary_parts[j],\n",
    "                horizon,\n",
    "                max_iterations)\n",
    "            \n",
    "    return (real_parts, imaginary_parts, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_numba = %timeit -o mandelbrot_set_numba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.add_time(times_numba, 'numba', 'gold').speedup_plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Cython](http://cython.org/) implementation fairly closely follows the simple and Numba versions, however there are several  points worth noting:\n",
    "* The `%%cython` cell magic, telling Jupyter that this is a Cython code cell.\n",
    "* The code requires the most changes. Unlike the Numba code, you can't just comment out a single line and revert to pure Python.\n",
    "* Explicit types are used for **all** variables. Cython allows you to use implicit types, but it defaults to an undifferentiated Python object. This nearly always results in slower code.\n",
    "* Use of the `@cython.boundscheck(False)` decorator to turn off array bounds-checking for the entire function. This provides a modest speed improvement.\n",
    "* Use of the `@cython.wraparound(False)` decorator to turn off negative array index wrapping for entire function. This provides a modest speed improvement. This can only be used if you never use negative indicies in the code.\n",
    "* Complex numbers are explicitly split into separate real and imaginary parts, this gave much better performance.\n",
    "* The per-pixel function is declared inline with an explicit return value. This gives a huge performance improvement.\n",
    "* The three ndarrays have explicit dimensions and types specified in the Cython type definition. For arrays that will only be accessed by simple integer indicies this allows Cython to use pointer arithmetic for very fast data access. You can't use this approach if more complex slicing is used.\n",
    "* Although Cython does allow default function arguments, it only appeared to work with literal values. I couldn't get it to work with the approach used on all the other versions.\n",
    "* This code took a lot longer to write and tune when compared to any of the other versions.\n",
    "\n",
    "Try changing some of the code options and re-running the timing test to explore the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "cdef inline long mandelbrot_cython(float c_real, float c_imag, float horizon, long max_iterations):\n",
    "    cdef float z_real = 0.0, z_real_sq = 0.0\n",
    "    cdef float z_imag = 0.0, z_imag_sq = 0.0\n",
    "    cdef long n = 0\n",
    "    cdef float horizon_sq = horizon * horizon\n",
    "    for n in range(max_iterations):\n",
    "        # precompute some shared values\n",
    "        z_real_sq = z_real * z_real\n",
    "        z_imag_sq = z_imag * z_imag\n",
    "        # Escape check\n",
    "        if z_real_sq + z_imag_sq > horizon_sq:\n",
    "            return n \n",
    "        z_imag = 2 * z_real * z_imag + c_imag\n",
    "        z_real = z_real_sq - z_imag_sq + c_real\n",
    "        \n",
    "    return 0\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def mandelbrot_set_cython(\n",
    "    float r_min,\n",
    "    float r_max,\n",
    "    float i_min,\n",
    "    float i_max,\n",
    "    long r_samples,\n",
    "    long i_samples,\n",
    "    float horizon,\n",
    "    long max_iterations):\n",
    "    \n",
    "    # Create a linearly-spaced sample of single-precision points in the complex plane\n",
    "    cdef np.ndarray[np.float32_t, ndim=1] real_parts = np.linspace(r_min, r_max, r_samples, dtype=np.float32)\n",
    "    cdef np.ndarray[np.float32_t, ndim=1] imaginary_parts = np.linspace(i_min, i_max, i_samples, dtype=np.float32)\n",
    "    cdef np.ndarray[np.int32_t, ndim=2] num_iterations = np.zeros((r_samples, i_samples), dtype=np.int32)\n",
    "        \n",
    "    for i in range(r_samples):\n",
    "        for j in range(i_samples):\n",
    "            num_iterations[i, j] = mandelbrot_cython(\n",
    "                real_parts[i],\n",
    "                imaginary_parts[j],\n",
    "                horizon,\n",
    "                max_iterations)\n",
    "            \n",
    "    return (real_parts, imaginary_parts, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_cython = %timeit -o mandelbrot_set_cython(default_r_min, default_r_max,default_i_min, default_i_max,default_r_samples, default_i_samples,default_horizon, default_max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.add_time(times_cython, 'Cython', 'sienna').speedup_plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "We have explored a lot of options for accelerating Python code. But what is the right method for your particular application? Unfortunately, there doesn't appear to be an answer that is right for all situations. If I had chosen a different problem, or even different parameters for this algorithm then the relative results may vary substantially.\n",
    "\n",
    "Vectorised NumPy code will provide performance that is fast enough in most scientific applications, with the added advantage of good portability. Numba and Cython can be hard to install if you are not using Anaconda, so they may make code hard to share with others or to use in a library.\n",
    "\n",
    "If the default NumPy performance is not sufficient, then NumExpr is particularly easy to test, as the code is quite similar to NumPy ufunc-based implementations. However the relative performance of NumExpr seems particularly sensitive to interactions between the array size and system cache. On my desktop, NumExpr nearly always outperforms NumPy but on Pearcey NumExpr does worse unless the array size is very large. But if I reduce the image size to 300x300 or less, the NumExpr version becomes slower than the Numpy version.\n",
    "\n",
    "Numba can frequently outperform NumPy on loop-centric functions, but it imposes a lot more limitations on the Python language features that can be used. You may also need to rewrite your code to use explicit loops. And as we have seen, that is not usually a good idea. Another use of Numba that was not explored here is using it to implement your own custom NumPy ufuncs. This feature can be particularly powerful.\n",
    "\n",
    "Cython can often provide the most performance benefit, but it also requires the most additional work. It also leads to code that bears the least resemblance to Python. However, when performance is critical, this may be a simpler option that writing directly in C, C++, or Fortran.\n",
    "\n",
    "It is worth noting that although the relative performance of NumExpr is quite variable in response to the array size and the number of iterations, Numba and Cython consistently outperform all other implementations of this algorithm.\n",
    "\n",
    "As always, the performance improvements should be guided by measurement and profiling. But what about the profiling we did here? Didn't it point to the inner function as the main bottle neck? Yes, but it is important to interpret the profiling results in light of what we know about the code. In this case, optimising both the outer and inner functions together allowed optimisations of the loop code as well as the inner per-pixel function. You can explore this with the Numba and Cython implementations by passing the optimised inner function to the original `mandelbrot_set` outer function.\n",
    "\n",
    "## Diminishing Returns: Why *Fast Enough* is often *Fast Enough*\n",
    "While the speedup plot we have been using is useful for understanding the relative performance of different optimisation approaches, it can give a misleading impression of the practical effect on job runtimes. To examine this, lets first look at the plot of walltimes from our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.walltime_plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, moving to a NumPy implementation led to a dramatic reduction in total execution time (the walltime). But despite all our additional efforts, and the impressive speedup numbers, each successive optimisation only reduced the execution time by a relatively small amount. Sometimes *fast enough is fast enough*.\n",
    "\n",
    "Lets illustrate this further by scaling our timing results to see how long each job would take if we assume:\n",
    "* That the original `simple` version actually took 12 hours to complete.\n",
    "* The relative speedup obtained was the same as our current results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assumed_walltime_minutes_slow_code = 12 * 60\n",
    "scale = assumed_walltime_minutes_slow_code / results.times[0]\n",
    "new_times = [t * scale for t in results.times]\n",
    "plt.scatter(x=range(len(new_times)), y=new_times)\n",
    "max_len = max((len(l) for l in results.labels))\n",
    "for t, l in zip(new_times, results.labels):\n",
    "    padding = ' ' * (max_len - len(l) + 1)\n",
    "    print('{0}:{2}{1:>6.2f} minutes'.format(l, t, padding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in our scenario, switching to NumPy would reduce the run time from 12 hours to around 15 minutes (or even less with the magnitude squared optimisation). As impressive as the additional speedup from Cython is, absolute reduction in run time is smaller than the difference between the simple and Numpy versions. Whether those extra minutes are worth the additional programming effort is an open question, with a different answer for every application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Image\n",
    "Now that we have some decent performance, it is possible to explore ...\n",
    "\n",
    "* Changing the [colormap](https://matplotlib.org/examples/color/colormaps_reference.html)\n",
    "* Changing the gamma value\n",
    "* Changing the image dimensions (`dpi`, `width`, `height`)\n",
    "* Changing the region of the complex plane that is calculated (`r_min`, `r_max`, `i_min`, and `i_max`). Note that for the Mandelbrot set, all the interesting things happen within a radius of 2 around the origin.\n",
    "\n",
    "To get started, let's try looking at $(-0.749, 0.065)$ to $(-0.748, 0.066)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 12\n",
    "height = 12\n",
    "dpi = 72\n",
    "num_pixels_x = dpi * width\n",
    "num_pixels_y = dpi * height\n",
    "\n",
    "image_results = mandelbrot_set_numba(\n",
    "    r_samples=num_pixels_x,\n",
    "    i_samples=num_pixels_y,\n",
    "    r_min=-0.749, r_max=-0.748,\n",
    "    i_min=0.065, i_max=0.066,\n",
    "    horizon=default_horizon, max_iterations=3000)\n",
    "\n",
    "render(\n",
    "    image_results, \n",
    "    image_width=width, image_height=height, \n",
    "    r_min=0.4, r_max=0.7,\n",
    "    i_min=0.3, i_max=0.6,\n",
    "    dpi=dpi, gamma=0.8,\n",
    "    cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easter Egg: ASCII Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Render_ASCII(iterations):\n",
    "    charset = ' .oXM@jawrpzgOQEPGJ'\n",
    "    width, height = iterations.shape\n",
    "    wrap = len(charset) - 1\n",
    "    for y in range(height):\n",
    "        row = [charset[int(i % wrap)] for i in iterations[...,y]]\n",
    "        print(''.join(row))\n",
    "\n",
    "image_results = mandelbrot_set_numba(r_samples=100, i_samples=40)\n",
    "Render_ASCII(image_results[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* [NumPy](http://www.numpy.org/)\n",
    "* [NumExpr](https://github.com/pydata/numexpr/wiki/Numexpr-Users-Guide)\n",
    "* [Numba](http://numba.pydata.org)\n",
    "* [Cython](http://cython.org/)\n",
    "* [Mandelbrot Set](https://en.wikipedia.org/wiki/Mandelbrot_set)\n",
    "* [Matplotlib Showcase](http://matplotlib.org/examples/showcase/mandelbrot.html)\n",
    "* Jean Francois Puget [How To Quickly Compute The Mandelbrot Set In Python](https://www.ibm.com/developerworks/community/blogs/jfp/entry/How_To_Compute_Mandelbrodt_Set_Quickly?lang=en)\n",
    "* Jean Francois Puget [My Christmas Gift](https://www.ibm.com/developerworks/community/blogs/jfp/entry/My_Christmas_Gift?lang=en)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
