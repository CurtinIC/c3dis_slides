{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and Visualising Data\n",
    "\n",
    "The Bureau of Meteorology (BoM) website includes a handy map tool which allows users to zoom in to locations, pick a data type, and then view the data on a subsequent page. That tool is shown in the image below. You can view the page at the following URL:\n",
    "\n",
    "http://www.bom.gov.au/climate/data/\n",
    "\n",
    "While this process is easy enough to do manually, if you wish to download multiple data sets then it will quickly become tedious. We want to automate this!\n",
    "\n",
    "In this tutorial, we will:\n",
    "- Explore and understand the BoM website URL parameters\n",
    "- Build a URL to download a web page\n",
    "- Harvest a URL from that page to download a zipped data set\n",
    "- Unpack the data set and extract a file\n",
    "- Visualise the data using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "This notebook contains a lot of code and explanatory text. But if you wanted to build a streamlined dashboard to explore weather station data, you could extract the code into a separate Python file and just include the GUI and plots into a notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/BOM_Map.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Dependencies and Configuration\n",
    "\n",
    "Import the packages required for this workshop, and add a little bit of configuration as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # building data frames from text\n",
    "from requests import get  # facilitate HTTP GET request\n",
    "from lxml import etree # harvest data from HTML\n",
    "from io import BytesIO # reading zip data to a byte stream for ZipFile to interpret\n",
    "from io import StringIO # reading weather data to a string stream for Pandas to interpret\n",
    "from zipfile import ZipFile # manipulate the downloaded ZIP file\n",
    "\n",
    "# IPython dependencies to facilitate dropdown handling\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import widgets\n",
    "#from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure matplotlib to use the inline rendering backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Web Resources\n",
    "\n",
    "Uniform Resource Locators (URLs) often contain structures which can be deconstructed from a few minutes of experimentation in your favorite web browser. For this workshop, we will be investigating weather data provided by the [Bureau of Meteorology](http://www.bom.gov.au/) (BoM).\n",
    "\n",
    "The BoM makes zip files of their weather stations across multiple data types available on their website. We will use a python library called 'requests' in order to download these files into python. An example URL is shown below:\n",
    "\n",
    "http://www.bom.gov.au/jsp/ncc/cdio/wData/wdata?p_nccObsCode=193&p_display_type=dailyDataFile&p_stn_num=015623&p_startYear=\n",
    "\n",
    "The above URL represents the page below. What are we looking at? It appears to represent Daily Rainfall data for the Melbourne (Olympic Park) weather station.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/BOM_Daily.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you visit the above page, you will notice that there is a lot of text data. While we could manually parse this data if necessary, there is a better alternative, as highlighted in the below image. The following URL is the red circled link shown below:\n",
    "\n",
    "```\n",
    "http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_display_type=dailyZippedDataFile&p_stn_num=086338&p_c=-1490905306&p_nccObsCode=136&p_startYear=2017\n",
    "```\n",
    "\n",
    "At that URL, a ZIP file is available, which contains all of the available Daily Rainfall data for the Melbourne (Olympic Park) station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "You don't need to download this file now. The goal of this notebook is to develop a flexible way of downloading archived data for any weather station.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/BOM_Daily_Annotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Sense of The URLs\n",
    "The first step in building our rainfall archive tool is to understand the request parameters of the main weather station page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Station Page URL\n",
    "\n",
    "Inspect the URL of the weather station page we have visited:\n",
    "\n",
    "http://www.bom.gov.au/jsp/ncc/cdio/wData/wdata?p_nccObsCode=136&p_display_type=dailyDataFile&p_stn_num=086338&p_startYear=\n",
    "\n",
    "There are some interesting parameters which are included with the URL:\n",
    "- `p_nccObsCode=136`\n",
    "- `p_display_type=dailyDataFile`\n",
    "- `p_stn_num=086338`\n",
    "- `p_startYear=`\n",
    "\n",
    "We can see two important numbers in this URL - `p_stn_num` and `p_nccObsCode`. What do these numbers mean? Normally, you would have to work to understand the URL structure, however today much of that work is summarised for you below. Here are two important numbers to consider in order to unlock the data scraping potential of the BoM's weather datasets:\n",
    "\n",
    "### Station ID ( p_stn_num )\n",
    "This is a six-digit zero-padded number that uniquely identifies the weather station. Some examples include:\n",
    "- 086338: Melbourne (Olympic Park)\n",
    "- 070247: Canberra (Australian National Botanic Gardens)\n",
    "- 015590: Alice Springs Airport\n",
    "\n",
    "### Observation Code ( p_nccObsCode )\n",
    "A three-digit (maybe zero-padded?) number that uniquely identifies the weather type:\n",
    "- 122: Daily Maximum Temperature (degrees celsius)\n",
    "- 123: Daily Minimum Temperature (degrees celsius)\n",
    "- 136: Daily Rainfall (mm)\n",
    "- 193: Daily Global Solar Exposure (MJ/m^2)\n",
    "\n",
    "**Combinations of the Station ID and the Observation Code will be used in this workshop to gather the data sets.**\n",
    "\n",
    "### Starting Year (p_startYear)\n",
    "<div class=\"alert alert-info\">\n",
    "The name `p_startYear` suggests that it provides a starting year for the requested data set. The fact that our URL works without a value suggests that this value is optional, and the logical default action is to request all years. We won't be using the start year in this notebook, but feel free to experiment on your own. One idea is to extend the `make_indirect_url()` function to accept an optional starting year and build it into the URL string. You could then add a GUI widget to select the required starting year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct ZIP File URL\n",
    "\n",
    "The BoM makes zip files of weather station data available with the following request structure:\n",
    "\n",
    "http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_display_type=dailyZippedDataFile&p_stn_num=086338&p_c=-1490905306&p_nccObsCode=136&p_startYear=2017\n",
    "\n",
    "While the above URL is quite long, in reality only a handful of characters will change between. As you can see, there are multiple parameters to this URL as well, including:\n",
    "- `p_display_type=dailyZippedDataFile`\n",
    "- `p_stn_num=086338`\n",
    "- `p_c=-1490905306`\n",
    "- `p_nccObsCode=136`\n",
    "- `p_startYear=2017`\n",
    "\n",
    "While it is obvious that `p_stn_num` and `p_nccObsCode` are the station ID and observation code that we have already identified, the other parameters are unclear. For example, `p_c` does not have a clear relation to the other numbers (though it may be related to a timestamp). This means we can not build the direct ZIP download URL to harvest the required ZIP files in a single step. But we do know enough to build the URL of the information page containing the ZIP download links (the indirect URL).\n",
    "\n",
    "**Therefore, through the magic of web scraping, we will use the indirect URL to lead us to the direct URL!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Station ID and Observation Code Values\n",
    "Before we can request and scrape a weather station page, we need to understand the valid values for the station ID and observation code. We need those values to build the page request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station ID\n",
    "Rather than figuring out the Station IDs manually by visiting the BoM map, a data set containing all of the station IDs and associated meta data is available at the following page:\n",
    "\n",
    "http://www.bom.gov.au/climate/cdo/about/sitedata.shtml\n",
    "\n",
    "The URL is difficult to find! Here is the direct link:\n",
    "\n",
    "`ftp://ftp.bom.gov.au/anon2/home/ncc/metadata/sitelists/stations.zip`\n",
    "\n",
    "This data set has already been downloaded, and an excerpt of the data set is shown in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frame = pd.read_csv('StationsComplete.csv', index_col=0)\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Note that the file headers have been adjusted slightly in order to simplify reading the data set into Pandas. If you are curious you could download the original file and compare it, although this is not required for this notebook exercise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have specified the first column as the index (the Weather Station ID), we can view a particular station by specifying the Weather Station ID, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_melbourne = 86338 # The Melbourne (Olympic Park) Station ID\n",
    "\n",
    "frame.loc[[station_melbourne]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time, the detective work for the observation codes has been done for you! This value determines the type of observation data being requested. There are only a few potential numerical values which we give to you now. \n",
    "\n",
    "To help with the web scraping GUI that we build later, lets define the available codes as a dictionary, using a human readable name as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode_mapper = {\n",
    "    'Maximum temperature (Degree C)': '122',\n",
    "    'Minimum temperature (Degree C)': '123',\n",
    "    'Rainfall amount (millimetres)': '136',\n",
    "    'Daily global solar exposure (MJ/m*m)': '193'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the two essential request parameters, we can move on to building the weather station page URL and scraping the ZIP download page URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the Weather Station Page via the Indirect URL\n",
    "A function to build the URL that contains both the Station ID and Observation Code is below.\n",
    "\n",
    "Note that the Station ID will also be zero-padded to six characters here, as it is necessary for the BoM web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_indirect_url(station, mode):\n",
    "    # creates a URL for indirect access to BoM data set resources\n",
    "    url_string = ('http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?'\n",
    "                  'p_nccObsCode={mode}&'\n",
    "                  'p_display_type=dailyDataFile'\n",
    "                  '&p_startYear='\n",
    "                  '&p_c='\n",
    "                  '&p_stn_num={station:06d}')\n",
    "    return url_string.format(station=int(station), mode=mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `requests.get()` function, we now define a convenience method to take a URL and retrieve the data from this URL. This function will be used twice in this workshop:\n",
    "1. Downloading the indirect URL to harvest the direct URL (if it is present)\n",
    "2. Downloading the ZIP file data to store in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_url_content(url):\n",
    "    # while this function is just two lines, it helps to separate out common processes\n",
    "    # where particular steps may be forgotten. In this instance, the 'content' property\n",
    "    # of response may be forgotten when downloading\n",
    "    response = get(url)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesting the Direct URL\n",
    "We now have the HTML page content from the indirect URL. Now we need to mine the HTML in order to find the data archive URL.\n",
    "\n",
    "There are many techniques and libraries for extracting information from HTML data. [Regular expressions](https://en.wikipedia.org/wiki/Regular_expression) and Python string searching functions can be used to look for particular text patterns, while the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) library provides a full suite of tools for web scraping.\n",
    "\n",
    "Here we are going to use a third approach. Exploiting the structure of HTML, we will use what is termed an 'XPath' to specify the location of a specific item in the HTML. An example is shown below:\n",
    "\n",
    "`//*[@id=\"content-block\"]/ul[2]/li[2]/a/@href`\n",
    "\n",
    "What does this mean? And how did we gather this? The HTML page is made up of tags such as `<div></div>` (a generic container), `<li></li>` (a list element), and `<a></a>` (a hyperlink). The above XPath is a set of instructions to traverse the HTML document in the following order:\n",
    "1. `//*[@id=\"content-block\"]` : select all elements on the page with an id of 'content-block' (assumed to yield just one entry)\n",
    "2. `/ul[2]` : select the second unordered list tag\n",
    "3. `/li[2]` : select the second list entry tag\n",
    "4. `/a` : select the hyperlink\n",
    "5. `/@href` : select the 'href' tag on the hyperlink (`<a>`)\n",
    "\n",
    "For this workshop, finding the required XPath is quite simple. Using Chrome:\n",
    "1. Right click on the 'All years of data' URL, and click 'Inspect'. The Chrome developer tools will appear below or at the right of the browser window.\n",
    "2. Right click on the highlighted element (coloured blue as shown below), and hover over 'Copy'\n",
    "3. Click on 'Copy XPath'. This will add the XPath to your clipboard.\n",
    "4. Paste the XPath in to your available editor, and add `/@href` to indicate we want the URL fragment from the hyperlink.\n",
    "\n",
    "Often more complex handling is needed where pages contain a less predictable structure. Thank you for the consistency, BoM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/BOM_XPath.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "If you feel confident, or just curious, feel free to [open this page](http://www.bom.gov.au/jsp/ncc/cdio/weatherData/av?p_nccObsCode=122&p_display_type=dailyDataFile&p_startYear=&p_c=&p_stn_num=086338) in a new tab and recreate the XPath discovery method.\n",
    "\n",
    "If you use Chrome, you may follow the method exactly. Most other browsers have comparable developer tools for inspecting the page [DOM](https://en.wikipedia.org/wiki/Document_Object_Model), but the steps will vary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Web Scraping Function\n",
    "Let's put the XPath of the ZIP archive download link to work by defining a function to retrieve the link from the page HTML. We are using the [lxml](http://lxml.de/) library to query the HTML DOM with our XPath before building the full URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_direct_url(html):\n",
    "    try:\n",
    "        # take the raw HTML string and convert it into something \n",
    "        # parseable by our python code\n",
    "        html_parsed = etree.HTML(html)\n",
    "        \n",
    "        # search the parsed HTML for the XPath we have created for the BoM data page\n",
    "        filtered = html_parsed.xpath('//*[@id=\"content-block\"]/ul[2]/li[2]/a/@href')\n",
    "        \n",
    "        if filtered[0]:\n",
    "            # if found, the URL is only a relative URL, so we have to attach\n",
    "            # the BoM domain in order for it to be a complete URL\n",
    "            return 'http://www.bom.gov.au{0}'.format(filtered[0])\n",
    "        \n",
    "        else:\n",
    "            # if nothing is found, then we pass None back to the caller\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        # simply return nothing, as there were issues with the provided html\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GUI for Selecting Weather Station and Observation Type\n",
    "\n",
    "Right, we now have all the code to find the data archive URL from a given station ID and observation code. Let's use the [IPyWidgets](http://ipywidgets.readthedocs.io/en/latest/) library to build a GUI to streamline selection of those two values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station Search\n",
    "\n",
    "When building this workshop, it was obvious that searching the text on the Site Name column was the simplest method of interaction. Type in the name of local towns in your neighbourhood to discover if BoM has installed a station there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we use the `IPython.display.clear_output()` function to animate the search results into the output cell. This function clears the output cell and replaces it with the new output. It's a simple but powerful technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_display = ['Site Name', 'Start', 'End']\n",
    "\n",
    "def search_site_name(value):\n",
    "    print(frame[frame['Site Name'].str.contains(value.upper())][columns_display])\n",
    "\n",
    "def change_search(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        clear_output(wait=True)\n",
    "        search_site_name(change.new)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Create text widget for input\n",
    "input_text = widgets.Text(description='Search Site Names', value='Melbourne')\n",
    "input_text.observe(change_search)\n",
    "display(input_text)\n",
    "\n",
    "# perform the initial run to demonstrate an example entry\n",
    "search_site_name(input_text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Once you have found an interesting station, enter its ID in the input below for validation and to assign it for the download URL.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the default selection to the 'Melbourne (Olympic Park)' Station ID\n",
    "station_selected = 86338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search_site_id(value):\n",
    "    global station_selected\n",
    "    if value and value.isdigit():\n",
    "        value = int(value)\n",
    "        if value in list(frame.index):\n",
    "            print('Station found for ID {0}'.format(value))\n",
    "            station_selected = int(value)\n",
    "            print(frame.loc[int(value)])\n",
    "        else:\n",
    "            print('No Station found for ID {0}'.format(value))\n",
    "            station_selected = None\n",
    "    else:\n",
    "        print('Please enter a Station ID (numerical)')\n",
    "        station_selected = None\n",
    "\n",
    "def change_search(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        clear_output(wait=True)\n",
    "        search_site_id(change.new)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Create text widget for input\n",
    "input_text = widgets.Text(description='Enter Site ID', value=str(station_selected))\n",
    "input_text.observe(change_search)\n",
    "display(input_text)\n",
    "\n",
    "# perform the initial run to demonstrate an example entry\n",
    "search_site_id(input_text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Type\n",
    "Since there is just a short list of observation types, we will use a Dropdown widget from IPyWidgets.\n",
    "\n",
    "The Dropdown widget accepts a dictionary. The keys are used for the display while the values are used as the selected value. However, when handling the change event of the Dropdown widget, we also want to go backwards from the selected value to the human-readable text. Unfortunately the Dropdown widget only let's you query the value, and Python dictionaries do not provide a reverse lookup from value to key. To get around this we build a second dictionary with the keys and values reversed. Note that this requires unique values for both value and key (*why?*).\n",
    "\n",
    "This approach can become error prone if the dictionaries change since you must remember to modify both dictionaries. If you have a dynamic dictionary that requires reverse lookups, consider a third-party library such as [`bidict`](https://pypi.python.org/pypi/bidict/0.3.1).\n",
    "\n",
    "An alternative approach would be to search the `mode_mapper` dictionary each time to find the human-readable text corresponding to the selected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refresh your memory, here is the `mode_mapper` dictionary we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the inverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_mode_mapper = dict([[v,k] for k,v in mode_mapper.items()])\n",
    "inverse_mode_mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, let's display the Dropdown widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arbitrarily select the first mode as our default selection\n",
    "mode_selected = mode_mapper[list(mode_mapper.keys())[0]]\n",
    "mode_pretty = inverse_mode_mapper[mode_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_mode(change):\n",
    "    global mode_selected\n",
    "    global mode_pretty\n",
    "    clear_output()\n",
    "    mode_selected = change.new\n",
    "    mode_pretty = inverse_mode_mapper[mode_selected]\n",
    "    print('Changed mode to {0} - {1}'.format(mode_selected, mode_pretty))\n",
    "\n",
    "# build the dropdown widget to manipulate the mode option\n",
    "dropdown = widgets.Dropdown(options=mode_mapper, value=mode_selected, description='Weather Data')\n",
    "dropdown.observe(change_mode, 'value')\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Note that not all data modes are available for each station.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together: Perform The ZIP Downloads\n",
    "Attempt to capture the direct URL with the functions we have created, then download the ZIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_indirect = make_indirect_url(station_selected, mode_selected)\n",
    "print('Built indirect URL - {0}'.format(url_indirect))\n",
    "indirect_content = download_url_content(url_indirect)\n",
    "print('Indirect URL contained HTML of {0} characters'.format(len(indirect_content)))\n",
    "url_direct = gather_direct_url(indirect_content)\n",
    "\n",
    "if url_direct != None:\n",
    "    print('Found URL {0}, downloading ZIP file...'.format(url_direct))\n",
    "    direct_content = download_url_content(url_direct)\n",
    "    print('ZIP file downloaded')\n",
    "else:\n",
    "    print('A URL for Station ID {0}, Mode {1} was not found!'.format(station_selected, mode_selected))\n",
    "    # Set direct_content so that subsequent cells will not produce results from last attempt\n",
    "    direct_content = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect The Zip File\n",
    "If a file was successfully download in the prior cell, lets continue to inspect the ZIP file and extract a CSV file if it is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this workshop, we do not save the file on the hard drive.\n",
    "# Instead, we read the data to StringIO which emulates a file\n",
    "zip_data = BytesIO()\n",
    "zip_data.write(direct_content)\n",
    "\n",
    "input_zip = ZipFile(zip_data)\n",
    "\n",
    "# gather the names of files in the ZIP to loop through\n",
    "file_names = input_zip.namelist()\n",
    "\n",
    "weather_data = None\n",
    "\n",
    "for entry in file_names:\n",
    "    # here we see if the file name contains '.csv'\n",
    "    # we have assumed that there is only one such 'Data.csv' file in the ZIP\n",
    "    if '.csv' in entry:\n",
    "        print('CSV File Found - {0}'.format(entry))\n",
    "        weather_data = input_zip.read(entry)\n",
    "        print('Read weather data of {0} bytes'.format(len(weather_data)))\n",
    "        \n",
    "if weather_data == None:\n",
    "    print('No CSV data set found in ZIP file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data into a Pandas DataFrame\n",
    "As the weather data for this workshop is all in-memory (rather than stored on the hard drive), there are a couple of extra steps needed to read the file into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding data from byte to utf-8\n",
    "weather_decoded = weather_data.decode(\"utf-8\")\n",
    "\n",
    "# sending the decoded data to a StringIO to emulate a file on the hard drive\n",
    "weather_file = StringIO()\n",
    "weather_file.write(weather_decoded)\n",
    "weather_file.seek(0)\n",
    "\n",
    "# read the CSV 'file' into a DataFrame \n",
    "weather_frame = pd.read_csv(weather_file)\n",
    "\n",
    "weather_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data\n",
    "Now that we have read the data, the next stage is to assign something more appropriate to the DataFrame index, and perform some basic data cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build a datetime index out of the Year, Month, and Day columns\n",
    "# it is assumed these columns are provided in the BoM data set\n",
    "weather_frame.index = pd.to_datetime(weather_frame[['Year', 'Month', 'Day']])\n",
    "\n",
    "weather_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can discard much of the provided data, besides the particular environmental data\n",
    "# column that we are interested in\n",
    "weather_cleaned = weather_frame[[mode_pretty]]\n",
    "\n",
    "# the BoM data may contain a large number of empty rows, as the weather station\n",
    "# may have started operation on May 10th, while the BoM data by default starts at Jan 1st\n",
    "# in addition, some rows throughout the data set may be empty due to unexpected downtime or\n",
    "# maintenance of the weather station\n",
    "weather_cleaned = weather_cleaned.dropna()\n",
    "\n",
    "# some rows may have been dropped in between rows that contain data.\n",
    "# after dropping the empty rows, now fix the indexing so that we again have a sequential data set\n",
    "idx = pd.date_range(weather_cleaned.index[0], weather_cleaned.index[-1])\n",
    "weather_cleaned = weather_cleaned.reindex(idx)\n",
    "\n",
    "# interpolate missing entries from rows after the reindexing\n",
    "# be aware that this strategy suits data with only a small number of consecutive\n",
    "# missing rows, and can produce undesired results if the data is sensitive to time or\n",
    "# behaves stochastically\n",
    "weather_cleaned[mode_pretty] = weather_cleaned[mode_pretty].interpolate().round(1)\n",
    "\n",
    "# Preview the data\n",
    "weather_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Data\n",
    "Once the data has been appropriately cleaned, we can then use Pandas to visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figsize = (16, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = weather_cleaned.plot(kind='line', legend=False, figsize=figsize)\n",
    "\n",
    "ax.set(xlabel=\"Year\", title=mode_pretty);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = weather_cleaned.groupby(weather_cleaned.index.year).mean().plot(legend=False, figsize=figsize)\n",
    "\n",
    "ax.set(xlabel=\"Year\", title=\"Weather Data - Yearly Mean\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = weather_cleaned.groupby(weather_cleaned.index.month).mean().plot(legend=False, figsize=figsize)\n",
    "\n",
    "ax.set(xlabel=\"Month\", title=\"Weather Data - Monthly Mean\");"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
